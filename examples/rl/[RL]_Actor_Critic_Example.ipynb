{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlkHDWgffQsSNakQhJJI3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/examples/rl/%5BRL%5D_Actor_Critic_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Tutorial: https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py\n",
        "\n",
        "Terminology\n",
        "- Actor == Policy\n",
        "- Critic == Value\n",
        "- Reward: the immediate feedback\n",
        "- Return: the total discounted reward,  starting from the current timestep."
      ],
      "metadata": {
        "id": "5zpid8uel_mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import argparse\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pqeb0OUxmHAd"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cart Pole Gym\n",
        "\n",
        "seed = 543\n",
        "gamma = 0.99\n",
        "log_interval = 10\n",
        "\n",
        "env = gymnasium.make('CartPole-v1')\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAdLgxYNmTMR",
        "outputId": "1f151c8f-d3e0-4529-88cd-c1b53c11b7de"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78abdff42530>"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Libs - Policy model\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  \"\"\"Implement both actor and critic in one model.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    hidden_dim = 128\n",
        "\n",
        "    # common layer\n",
        "    self.affine1 = nn.Linear(4, hidden_dim)   # 4 is the state space.\n",
        "\n",
        "    # actor's head\n",
        "    self.actor_head = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    # critic head\n",
        "    self.value_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    # action & reward buffer\n",
        "    self.saved_actions = []\n",
        "    self.rewards = []\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Forward for both actor and critic.\n",
        "\n",
        "    Args:\n",
        "      x: the current state. It should contain 4 floats to represent the state.\n",
        "    \"\"\"\n",
        "    assert x.shape == (4,)\n",
        "    x = self.affine1(x)     # [H]\n",
        "    x = F.relu(x)\n",
        "\n",
        "    action_logits = self.actor_head(x)\n",
        "    action_prob = F.softmax(action_logits, dim=-1)   # [2]\n",
        "\n",
        "    state_value = self.value_head(x)                  # [1]\n",
        "\n",
        "    return action_prob, state_value"
      ],
      "metadata": {
        "id": "LODlVZkRnAc7"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Libs - selection action\n",
        "\n",
        "bs = 2\n",
        "policy = Policy()\n",
        "state = torch.randn((bs, 4))\n",
        "state = np.random.randint((bs, 4))\n",
        "# output_action_probs, output_state_values = policy(input)\n",
        "\n",
        "# print(f\"{output_action_probs=}, {output_state_values=}\")\n",
        "\n",
        "def select_action(policy: Policy, state: int):\n",
        "  state = torch.from_numpy(state).float()\n",
        "  action_prob, state_value = policy(state)\n",
        "\n",
        "  # Sample from the probs and emit the enum int of the next action\n",
        "  sampler = Categorical(action_prob)\n",
        "  next_action = sampler.sample()               # [1]\n",
        "\n",
        "  # save the action to buffer\n",
        "  #\n",
        "  # log_prob is the log of the probability of the selected action.\n",
        "  log_prob = sampler.log_prob(next_action)     # [1]\n",
        "  assert torch.allclose(log_prob, action_prob[next_action].log(), atol=1e-5)\n",
        "  policy.saved_actions.append(SavedAction(log_prob, state_value))\n",
        "\n",
        "  return next_action.item()\n",
        "\n",
        "state, _ = env.reset()\n",
        "next_action = select_action(policy, state)\n",
        "print(f\"{next_action=}, prob={policy.saved_actions[-1].log_prob.exp():.3f}, pred value={policy.saved_actions[-1].value.item():.3f}\")\n",
        "print(f\"# saved actions = {len(policy.saved_actions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeFCQuRuPBBS",
        "outputId": "1fa0defb-ec8e-4d59-d45e-829ee21d3e77"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_action=1, prob=0.499, pred value=0.085\n",
            "# saved actions = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Question - In the training loop below, it does **many FWD passes** to collect a full trajectory, and then do a **single backprop** for all these fwd passes together. How is this possible this is not 1-to-1 mapping? Specifically, each **FWD pass' activations** need to be remembered to be used in backprop to calculate **grad**, but will a later FWD overwrite its previous FWD's activation?\n",
        "\n",
        "> Yes this works! The **activations from FWD passes** are stored in what's called a **computation graph**. When you do FWD or loss calculation, these operations (including activation) are stored into the CG; when you do **multiple FWDs** and accumulate their results (like summing up the losses) into a single tensor, PyTorch extends the **same computation graph**. During backprop, the stored activations are used to calculate the grad. And importantly, when loss.backward() is called, PyTorch **destroys this entire computation graph** by default to free up memory.\n"
      ],
      "metadata": {
        "id": "i3SeaDIeg1Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finish_episode(optimizer, policy: Policy, gamma: float, eps=np.finfo(np.float32).eps.item()):\n",
        "  \"\"\"Training code. Calculates actor and critic loss and performs backprop.\n",
        "\n",
        "  Args:\n",
        "    policy: the policy model.\n",
        "    gamma: The discount factor (gamma) determines how much an agent prioritizes\n",
        "      future rewards over immediate ones.\n",
        "  \"\"\"\n",
        "\n",
        "  # 1. Init vars\n",
        "  saved_actions = policy.saved_actions\n",
        "  actor_losses = []       # Save the actor (policy) losses\n",
        "  critic_losses = []      # Save the critic (value) losses\n",
        "\n",
        "  # Reward: for a single step\n",
        "  # Return: for the aggregated\n",
        "  R = 0                   # Return for a single trajectory\n",
        "  returns = []            # List to save the true values for a single trajectory\n",
        "  n_steps = len(policy.rewards)\n",
        "\n",
        "  # 2. Calculate the true value using rewards returned from the env\n",
        "  for r in policy.rewards[::-1]:  # iterate backwards\n",
        "    # calculate the discounted value\n",
        "    R = r + gamma * R\n",
        "    returns.insert(0, R)\n",
        "\n",
        "  returns = torch.tensor(returns)\n",
        "  returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "  # print(f\"{returns=}\")\n",
        "\n",
        "  assert len(returns) == len(policy.saved_actions)\n",
        "\n",
        "  # 3. Collect losses from the full trajectory\n",
        "  for (log_prob, value), R in zip(policy.saved_actions, returns):\n",
        "    advantage = R - value.item()\n",
        "\n",
        "    # actor loss\n",
        "    actor_losses.append(-1 * log_prob * advantage)\n",
        "\n",
        "    # critic loss\n",
        "    critic_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
        "\n",
        "  # 4. Backprop\n",
        "  optimizer.zero_grad()\n",
        "  loss = torch.stack(actor_losses).sum() + torch.stack(critic_losses).sum()\n",
        "  loss_val = loss.detach().item()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 5. Clear the rewards and action buffer\n",
        "  del policy.rewards[:]\n",
        "  del policy.saved_actions[:]\n",
        "\n",
        "  return loss_val, n_steps\n",
        "\n",
        "# optimizer = optim.Adam(policy.parameters(), lr=3e-2)\n",
        "# eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# finish_episode(optimizer=optimizer, policy=policy, gamma=0.9, eps=eps)\n"
      ],
      "metadata": {
        "id": "vSoO8N08WHz1"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "jUn8bNwWYVcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "running_reward = 0\n",
        "\n",
        "policy = Policy()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=3e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "reward_threshold = env.spec.reward_threshold\n",
        "\n",
        "print(f\"Start running... Target reward threshold: {reward_threshold}\")\n",
        "\n",
        "# run infinitely many episodes, until the reward threshold is met.\n",
        "for i_episode in count(1):\n",
        "  # reset the env\n",
        "  state, _ = env.reset()\n",
        "  ep_reward = 0\n",
        "\n",
        "  # for each episode, only run 9999 steps to avoid infinite loop\n",
        "  for t in range(1, 10000):\n",
        "    action = select_action(policy=policy, state=state)\n",
        "\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "    policy.rewards.append(reward)\n",
        "    ep_reward += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  # Backprop\n",
        "  loss, n_steps = finish_episode(optimizer=optimizer, policy=policy, gamma=gamma, eps=eps)\n",
        "\n",
        "  if i_episode % log_interval == 0:\n",
        "    print(f\"Episod: {i_episode}, Total Loss: {loss:.3f}, Avg Loss: {loss/n_steps:.3f}, Running Return: {running_reward:.3f}\")\n",
        "\n",
        "  running_reward = 0.05 * ep_reward + 0.95 * running_reward\n",
        "  if running_reward > reward_threshold:\n",
        "    print(f\"Solved after {i_episode} episods! Running reward: {running_reward} > reward threshold: {reward_threshold}\")\n",
        "    break\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0yfFrIewuAR",
        "outputId": "8314fe15-1431-4ecd-aee4-b90728a35d88"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running... Target reward threshold: 475.0\n",
            "Episod: 10, Total Loss: 1.802, Avg Loss: 0.225, Running Return: 4.478\n",
            "Episod: 20, Total Loss: 0.215, Avg Loss: 0.027, Running Return: 6.406\n",
            "Episod: 30, Total Loss: 0.247, Avg Loss: 0.031, Running Return: 7.524\n",
            "Episod: 40, Total Loss: 0.083, Avg Loss: 0.009, Running Return: 8.144\n",
            "Episod: 50, Total Loss: 0.236, Avg Loss: 0.030, Running Return: 8.609\n",
            "Episod: 60, Total Loss: 0.328, Avg Loss: 0.033, Running Return: 8.757\n",
            "Episod: 70, Total Loss: 0.062, Avg Loss: 0.007, Running Return: 9.109\n",
            "Episod: 80, Total Loss: 0.525, Avg Loss: 0.066, Running Return: 9.063\n",
            "Episod: 90, Total Loss: 0.355, Avg Loss: 0.044, Running Return: 9.253\n",
            "Episod: 100, Total Loss: 0.023, Avg Loss: 0.003, Running Return: 9.167\n",
            "Episod: 110, Total Loss: 0.176, Avg Loss: 0.018, Running Return: 9.288\n",
            "Episod: 120, Total Loss: 0.017, Avg Loss: 0.002, Running Return: 9.328\n",
            "Episod: 130, Total Loss: 0.853, Avg Loss: 0.071, Running Return: 9.377\n",
            "Episod: 140, Total Loss: 0.060, Avg Loss: 0.006, Running Return: 9.558\n",
            "Episod: 150, Total Loss: 0.150, Avg Loss: 0.019, Running Return: 9.498\n",
            "Episod: 160, Total Loss: 0.040, Avg Loss: 0.004, Running Return: 9.488\n",
            "Episod: 170, Total Loss: 0.149, Avg Loss: 0.015, Running Return: 9.485\n",
            "Episod: 180, Total Loss: 0.262, Avg Loss: 0.029, Running Return: 9.669\n",
            "Episod: 190, Total Loss: 0.729, Avg Loss: 0.081, Running Return: 10.195\n",
            "Episod: 200, Total Loss: 0.989, Avg Loss: 0.090, Running Return: 11.282\n",
            "Episod: 210, Total Loss: 1.782, Avg Loss: 0.064, Running Return: 12.279\n",
            "Episod: 220, Total Loss: 16.309, Avg Loss: 0.453, Running Return: 18.786\n",
            "Episod: 230, Total Loss: 6.777, Avg Loss: 0.133, Running Return: 39.264\n",
            "Episod: 240, Total Loss: 2.323, Avg Loss: 0.129, Running Return: 42.826\n",
            "Episod: 250, Total Loss: 10.671, Avg Loss: 0.281, Running Return: 40.328\n",
            "Episod: 260, Total Loss: 22.772, Avg Loss: 0.506, Running Return: 38.119\n",
            "Episod: 270, Total Loss: 20.466, Avg Loss: 0.359, Running Return: 42.239\n",
            "Episod: 280, Total Loss: 3.152, Avg Loss: 0.039, Running Return: 59.534\n",
            "Episod: 290, Total Loss: 24.615, Avg Loss: 0.270, Running Return: 88.044\n",
            "Episod: 300, Total Loss: 5.566, Avg Loss: 0.180, Running Return: 71.316\n",
            "Episod: 310, Total Loss: 27.065, Avg Loss: 0.601, Running Return: 60.229\n",
            "Episod: 320, Total Loss: 39.706, Avg Loss: 0.401, Running Return: 61.937\n",
            "Episod: 330, Total Loss: 20.287, Avg Loss: 0.286, Running Return: 70.394\n",
            "Episod: 340, Total Loss: 8.652, Avg Loss: 0.095, Running Return: 78.646\n",
            "Episod: 350, Total Loss: 1.663, Avg Loss: 0.012, Running Return: 94.192\n",
            "Episod: 360, Total Loss: 3.016, Avg Loss: 0.024, Running Return: 101.164\n",
            "Episod: 370, Total Loss: -2.392, Avg Loss: -0.021, Running Return: 105.712\n",
            "Episod: 380, Total Loss: 2.512, Avg Loss: 0.140, Running Return: 96.311\n",
            "Episod: 390, Total Loss: 6.644, Avg Loss: 0.063, Running Return: 86.192\n",
            "Episod: 400, Total Loss: 24.045, Avg Loss: 0.207, Running Return: 94.699\n",
            "Episod: 410, Total Loss: 2.904, Avg Loss: 0.014, Running Return: 101.160\n",
            "Episod: 420, Total Loss: 3.454, Avg Loss: 0.018, Running Return: 119.134\n",
            "Episod: 430, Total Loss: 42.780, Avg Loss: 0.185, Running Return: 134.092\n",
            "Episod: 440, Total Loss: -2.578, Avg Loss: -0.026, Running Return: 251.038\n",
            "Solved after 443 episods! Running reward: 502.4059650782717 > reward threshold: 475.0\n"
          ]
        }
      ]
    }
  ]
}