{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xwPm8GhC-UOL"
      ],
      "authorship_tag": "ABX9TyN8GHN30MaH3j8tZGPIbA93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/examples/pytorch_dist/%5BDist%5D_PyTorch_FSDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the instruction:\n",
        "https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html"
      ],
      "metadata": {
        "id": "GE-85zRa-4OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install the requirement\n",
        "# See https://github.com/pytorch/examples/blob/main/distributed/FSDP2/requirements.txt\n",
        "\n",
        "# %%capture\n",
        "# !pip install uv\n",
        "# !uv pip install torch==2.7.0\n",
        "\n",
        "!pip install torch==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWWHtrPaEJqu",
        "outputId": "cc4bfee1-21f9-457f-8c5e-6b6a37f178ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UJbJoyKh7XiR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.distributed.fsdp import fully_shard, FSDPModule"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Transfomer Model\n",
        "\n",
        "https://github.com/pytorch/examples/blob/main/distributed/FSDP2/model.py\n"
      ],
      "metadata": {
        "id": "xwPm8GhC-UOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    n_layers: int = 4\n",
        "    vocab_size: int = 8\n",
        "    max_seq_len: int = 16\n",
        "    dim: int = 16\n",
        "    n_heads: int = 4\n",
        "    dropout_p: float = 0.1\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        assert args.dim % args.n_heads == 0\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dropout_p = args.dropout_p\n",
        "        self.resid_dropout = nn.Dropout(args.dropout_p)\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, args.dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, args.dim, bias=False)\n",
        "        self.wo = nn.Linear(args.dim, args.dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz, seq_len, _ = x.size()\n",
        "        queries, keys, values = self.wq(x), self.wk(x), self.wv(x)\n",
        "        queries = queries.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
        "        keys = keys.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
        "        values = values.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        queries = queries.transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)\n",
        "        keys = keys.transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)\n",
        "        values = values.transpose(1, 2)  # (bsz, n_heads, seq_len, head_dim)\n",
        "\n",
        "        output = F.scaled_dot_product_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            None,\n",
        "            self.dropout_p if self.training else 0,\n",
        "        )\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
        "        return self.resid_dropout(self.wo(output))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.wq.reset_parameters()\n",
        "        self.wk.reset_parameters()\n",
        "        self.wv.reset_parameters()\n",
        "        self.wo.reset_parameters()\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout_p):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.w2 = nn.Linear(hidden_dim, dim)\n",
        "        self.resid_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resid_dropout(self.w2(self.gelu(self.w1(x))))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.w1.reset_parameters()\n",
        "        self.w2.reset_parameters()\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.attention_norm = nn.LayerNorm(args.dim)\n",
        "        self.attention = Attention(args)\n",
        "        self.ffn_norm = nn.LayerNorm(args.dim)\n",
        "        self.feed_forward = FeedForward(\n",
        "            args.dim, hidden_dim=4 * args.dim, dropout_p=args.dropout_p\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x + self.attention(self.attention_norm(x))\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.attention_norm.reset_parameters()\n",
        "        self.attention.reset_parameters()\n",
        "        self.ffn_norm.reset_parameters()\n",
        "        self.feed_forward.reset_parameters()\n",
        "\n",
        "\n",
        "# A toy transformer model, partly inspired by the nanoGPT model:\n",
        "# https://github.com/karpathy/nanoGPT.\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        assert args.vocab_size is not None\n",
        "        assert args.max_seq_len is not None\n",
        "        self.model_args = args\n",
        "        self.max_seq_len = args.max_seq_len\n",
        "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
        "        self.pos_embeddings = nn.Embedding(args.max_seq_len, args.dim)\n",
        "        self.dropout = nn.Dropout(args.dropout_p)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(args.n_layers):\n",
        "            self.layers.append(TransformerBlock(args))\n",
        "        self.norm = nn.LayerNorm(args.dim)\n",
        "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        _bsz, seq_len = tokens.size()\n",
        "        assert seq_len <= self.max_seq_len\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        pos = torch.arange(0, seq_len, device=tokens.device)\n",
        "        p = self.pos_embeddings(pos)  # positional embeddings of shape (seq_len, dim)\n",
        "        h = h + p\n",
        "        h = self.dropout(h)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h)\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h).float()\n",
        "        return output\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.tok_embeddings.reset_parameters()\n",
        "        self.pos_embeddings.reset_parameters()\n",
        "        self.norm.reset_parameters()\n",
        "        self.output.reset_parameters()"
      ],
      "metadata": {
        "id": "MHpXS7AZ7vHm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FSDP"
      ],
      "metadata": {
        "id": "kesH--af-aUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.tensor import DTensor, Shard\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "def run(rank, world_size):\n",
        "  \"\"\" Distributed function to do the real work.\"\"\"\n",
        "  device = \"cpu\"\n",
        "  max_seq_len = 16\n",
        "  vocab_size = 8\n",
        "\n",
        "  model_args = ModelArgs(max_seq_len=max_seq_len, vocab_size=vocab_size)\n",
        "  model = Transformer(model_args)\n",
        "\n",
        "  # Transform the model to FSDP\n",
        "  for layer in model.layers:\n",
        "    fully_shard(layer)\n",
        "  fully_shard(model)\n",
        "\n",
        "  # Verifications\n",
        "  assert isinstance(model, Transformer)\n",
        "  assert isinstance(model, FSDPModule)\n",
        "\n",
        "  for param in model.parameters():\n",
        "    assert isinstance(param, DTensor)\n",
        "    assert param.placements == (Shard(0), )\n",
        "\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "  batch_size = 4\n",
        "  seq_len = max_seq_len\n",
        "  for epoch in range(epochs):\n",
        "    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "\n",
        "    # This should be a loss example; no physical meaning.\n",
        "    loss = model(x) # [B, N, H]\n",
        "    loss = loss.sum() # [1]\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    if rank == 0:\n",
        "      print(f\"Finished {epoch=}\")\n",
        ""
      ],
      "metadata": {
        "id": "LmwVw8I281dR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Biolerplater to run the dist programs\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    print(f\"Initiated {backend=} {rank=} {world_size=}\")\n",
        "    fn(rank, size)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = 2\n",
        "    processes = []\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Running in Google Colab\")\n",
        "        mp.get_context(\"spawn\")\n",
        "    else:\n",
        "        mp.set_start_method(\"spawn\")\n",
        "    for rank in range(world_size):\n",
        "        p = mp.Process(target=init_process, args=(rank, world_size, run))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQVXP6z-W16",
        "outputId": "4350d4f1-a53a-473f-fbfb-c5fd553d84d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Initiated backend='gloo' rank=0 world_size=2\n",
            "Initiated backend='gloo' rank=1 world_size=2\n",
            "Finished epoch=0\n",
            "Finished epoch=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FSDP with Explicit Prefetching"
      ],
      "metadata": {
        "id": "5rwFxXj_aX6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.tensor import DTensor, Shard\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "def run_fsdp_explicit_prefetching(rank, world_size):\n",
        "  \"\"\" Distributed function to do the real work.\"\"\"\n",
        "  device = \"cpu\"\n",
        "  max_seq_len = 16\n",
        "  vocab_size = 8\n",
        "\n",
        "  model_args = ModelArgs(max_seq_len=max_seq_len, vocab_size=vocab_size)\n",
        "  model = Transformer(model_args)\n",
        "\n",
        "  # Transform the model to FSDP\n",
        "  for layer in model.layers:\n",
        "    fully_shard(layer)\n",
        "  fully_shard(model)\n",
        "\n",
        "  # Verifications\n",
        "  assert isinstance(model, Transformer)\n",
        "  assert isinstance(model, FSDPModule)\n",
        "\n",
        "  for param in model.parameters():\n",
        "    assert isinstance(param, DTensor)\n",
        "    assert param.placements == (Shard(0), )\n",
        "\n",
        "  # Explicit prefetching\n",
        "  #\n",
        "  # Users can specify forward ordering with set_modules_to_forward_prefetch, and\n",
        "  # backward ordering with set_modules_to_backward_prefetch. As shown in the\n",
        "  # code below, CPU thread issue all-gather i + 1 and i + 2 at layer i\n",
        "  num_to_forward_prefetch = 2\n",
        "  for i, layer in enumerate(model.layers):\n",
        "    if i >= len(model.layers) - num_to_forward_prefetch:\n",
        "      break\n",
        "    layers_to_prefetch = [\n",
        "        model.layers[i + j] for j in range(1, num_to_forward_prefetch + 1)\n",
        "    ]\n",
        "    layer.set_modules_to_forward_prefetch(layers_to_prefetch)\n",
        "\n",
        "  num_to_backward_prefetch = 2\n",
        "  for i, layer in enumerate(model.layers):\n",
        "    if i < num_to_backward_prefetch:\n",
        "        continue\n",
        "    layers_to_prefetch = [\n",
        "        model.layers[i - j] for j in range(1, num_to_backward_prefetch + 1)\n",
        "    ]\n",
        "    layer.set_modules_to_backward_prefetch(layers_to_prefetch)\n",
        "\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "  batch_size = 4\n",
        "  seq_len = max_seq_len\n",
        "  for epoch in range(epochs):\n",
        "    # trigger 1st all-gather earlier\n",
        "    # this overlaps all-gather with any computation before model(x)\n",
        "    model.unshard()\n",
        "\n",
        "    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "\n",
        "    # This should be a loss example; no physical meaning.\n",
        "    loss = model(x) # [B, N, H]\n",
        "    loss = loss.sum() # [1]\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    if rank == 0:\n",
        "      print(f\"Finished {epoch=}\")\n",
        ""
      ],
      "metadata": {
        "id": "Ab8p84gNacGM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Biolerplater to run the dist programs\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    print(f\"Initiated {backend=} {rank=} {world_size=}\")\n",
        "    fn(rank, size)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = 2\n",
        "    processes = []\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Running in Google Colab\")\n",
        "        mp.get_context(\"spawn\")\n",
        "    else:\n",
        "        mp.set_start_method(\"spawn\")\n",
        "    for rank in range(world_size):\n",
        "        p = mp.Process(target=init_process, args=(rank, world_size, run_fsdp_explicit_prefetching))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7ae8H_kbaiI",
        "outputId": "0fe3c500-01c2-44e0-a397-9dab98cc74e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Initiated backend='gloo' rank=0 world_size=2\n",
            "Initiated backend='gloo' rank=1 world_size=2\n",
            "Finished epoch=0\n",
            "Finished epoch=1\n",
            "Finished epoch=2\n",
            "Finished epoch=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXJkbcE2EDFc",
        "outputId": "17d1d206-af82-420b-a9b4-2a8b4207b8dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0+cu126\n"
          ]
        }
      ]
    }
  ]
}