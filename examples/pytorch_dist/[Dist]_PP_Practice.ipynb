{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC1B/EStbWynMPBqbsZS1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/examples/pytorch_dist/%5BDist%5D_PP_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems the main challenge for PP is to implement a scheduler/orchestrator.\n",
        "\n",
        "It is hard to find simple code example focusing on the data communication etc. (e.g. it is relatively easy to find for TP)\n",
        "\n",
        "Some good reads:\n",
        "- https://siboehm.com/articles/22/pipeline-parallel-training & https://github.com/siboehm/ShallowSpeed\n",
        "- https://torchgpipe.readthedocs.io/en/stable/"
      ],
      "metadata": {
        "id": "iIvzjXzzJJYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use PyTorch Library\n",
        "\n",
        "> Tutorial: https://docs.pytorch.org/tutorials/intermediate/pipelining_tutorial.html\n",
        "\n",
        "Note\n",
        "- The tutorial only discusses the **forward pass**.\n",
        "- TODO: find how to implement the **backward pass**."
      ],
      "metadata": {
        "id": "W5QnqfZWq4zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "   dim: int = 32\n",
        "   n_layers: int = 8\n",
        "   n_heads: int = 4\n",
        "   vocab_size: int = 100\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "   def __init__(self, model_args: ModelArgs):\n",
        "      super().__init__()\n",
        "\n",
        "      self.tok_embeddings = nn.Embedding(model_args.vocab_size, model_args.dim)\n",
        "\n",
        "      # Using a ModuleDict lets us delete layers witout affecting names,\n",
        "      # ensuring checkpoints will correctly save and load.\n",
        "      self.layers = torch.nn.ModuleDict()\n",
        "      for layer_id in range(model_args.n_layers):\n",
        "            self.layers[str(layer_id)] = nn.TransformerDecoderLayer(model_args.dim, model_args.n_heads)\n",
        "\n",
        "      self.norm = nn.LayerNorm(model_args.dim)\n",
        "      self.output = nn.Linear(model_args.dim, model_args.vocab_size)\n",
        "\n",
        "   def forward(self, tokens: torch.Tensor):\n",
        "      # Handling layers being 'None' at runtime enables easy pipeline splitting\n",
        "      h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n",
        "\n",
        "      for layer in self.layers.values():\n",
        "            h = layer(h, h)\n",
        "\n",
        "      h = self.norm(h) if self.norm else h\n",
        "      output = self.output(h).clone() if self.output else h\n",
        "      return output"
      ],
      "metadata": {
        "id": "9UwEy7wAq7De"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Partition the Transformer Model\n",
        "\n",
        "2 stages\n",
        "- Stage 1: the first few layers of the model\n",
        "- Stage 2: the last few layers of the model\n",
        "\n",
        "What is `PipelineStage`?\n",
        "\n",
        "> We need to create PipelineStage objects that wrap the part of the model running in that stage. The PipelineStage is responsible for **allocating communication buffers and creating send/recv ops to communicate with its peers**. It manages intermediate buffers e.g. for the outputs of forward that have not been consumed yet, and it provides a utility for running the backwards for the stage model."
      ],
      "metadata": {
        "id": "h735Y_sGyDLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.distributed as dist\n",
        "from torch.distributed.pipelining import pipeline, SplitPoint, PipelineStage, ScheduleGPipe\n",
        "\n",
        "def manual_model_split(\n",
        "    model: nn.Module, stage_index: int, num_stages: int, device: str) -> PipelineStage:\n",
        "  assert stage_index in (0, 1), f\"{stage_index=}\"\n",
        "\n",
        "  if stage_index == 0:\n",
        "    n_layers = len(model.layers)\n",
        "    for layer_id in range(n_layers // 2, n_layers):\n",
        "      del model.layers[str(layer_id)]\n",
        "    assert len(model.layers) == n_layers // 2\n",
        "\n",
        "    model.norm = None\n",
        "    model.output = None\n",
        "\n",
        "  else:\n",
        "    n_layers = len(model.layers)\n",
        "    for layer_id in range(n_layers // 2):\n",
        "      del model.layers[str(layer_id)]\n",
        "    assert len(model.layers) == n_layers // 2\n",
        "\n",
        "    model.tok_embeddings = None\n",
        "\n",
        "  stage = PipelineStage(\n",
        "      model,\n",
        "      stage_index,\n",
        "      num_stages,\n",
        "      device,\n",
        "  )\n",
        "  return stage"
      ],
      "metadata": {
        "id": "YwetiIr9yPjb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def init_process(rank: int, world_size: int):\n",
        "  assert world_size == 2\n",
        "\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  # Config\n",
        "  device = torch.device(f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  pp_group = dist.new_group()\n",
        "  stage_index = rank\n",
        "  num_stages = world_size\n",
        "  num_microbatches = 4\n",
        "  loss_fn = F.cross_entropy\n",
        "\n",
        "  # Model\n",
        "  torch.manual_seed(123)\n",
        "  model_args = ModelArgs()\n",
        "  model = Transformer(model_args)\n",
        "\n",
        "  # Input\n",
        "  x = torch.ones(32, 500, dtype=torch.long)   # [B, S]\n",
        "  y = torch.randint(0, model_args.vocab_size, (32, 500), dtype=torch.long)\n",
        "\n",
        "  # Partition\n",
        "  def tokenwise_loss_fn(outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    B, S, VOCAB_SIZE = outputs.shape\n",
        "    B_TARGET, S_TARGET = targets.shape\n",
        "    assert B == B_TARGET\n",
        "    assert S == S_TARGET\n",
        "\n",
        "    outputs = outputs.reshape(-1, VOCAB_SIZE)\n",
        "    targets = targets.reshape(-1)\n",
        "    return loss_fn(outputs, targets)\n",
        "\n",
        "  stage = manual_model_split(model, stage_index=stage_index, num_stages=num_stages, device=device)\n",
        "  schedule = ScheduleGPipe(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n",
        "\n",
        "  model.to(device)\n",
        "  x.to(device)\n",
        "  y.to(device)\n",
        "\n",
        "  # Step\n",
        "  if rank == 0:\n",
        "    schedule.step(x)\n",
        "  else:\n",
        "    losses = []\n",
        "    schedule.step(target=y, losses=losses)\n",
        "    losses = [l.item() for l in losses]\n",
        "    print(f\"{losses=}\")\n",
        "\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12359' # You can choose a different port if 12355 is in use\n",
        "\n",
        "world_size = 2\n",
        "\n",
        "processes = []\n",
        "for rank in range(world_size):\n",
        "  p = mp.Process(target=init_process, args=(rank, world_size))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyvcjz4gs9wR",
        "outputId": "50d605d4-db2e-45c7-c4ea-778084ef0550"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=2\n",
            "Starting process with rank=1, world_size=2\n",
            "losses=[4.805856704711914, 4.783386707305908, 4.796078205108643, 4.799017906188965]\n"
          ]
        }
      ]
    }
  ]
}