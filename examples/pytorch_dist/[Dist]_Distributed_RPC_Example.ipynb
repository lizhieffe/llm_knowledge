{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpIL/D3MU+ZN1xcYgNUIor",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/examples/pytorch_dist/%5BDist%5D_Distributed_RPC_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue: the init_rpc freezes in colab\n",
        "\n",
        "> Tutorial: https://docs.pytorch.org/tutorials/intermediate/rpc_tutorial.html\n",
        "\n",
        "> The RL example is based on: https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py"
      ],
      "metadata": {
        "id": "5zpid8uel_mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import argparse\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "Pqeb0OUxmHAd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cart Pole Gym\n",
        "\n",
        "seed = 543\n",
        "gamma = 0.99\n",
        "log_interval = 10\n",
        "\n",
        "env = gymnasium.make('CartPole-v1')\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "AGENT_NAME = \"agent\"\n",
        "OBSERVER_NAME=\"obs{}\""
      ],
      "metadata": {
        "id": "aAdLgxYNmTMR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Libs - Policy model\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  \"\"\"Implement both actor and critic in one model.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    hidden_dim = 128\n",
        "\n",
        "    # common layer\n",
        "    self.affine1 = nn.Linear(4, hidden_dim)   # 4 is the state space.\n",
        "    self.dropout = nn.Dropout(p=0.6)\n",
        "\n",
        "    # actor's head\n",
        "    self.actor_head = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    # critic head\n",
        "    self.value_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    # action & reward buffer\n",
        "    self.saved_actions = []\n",
        "    self.rewards = []\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Forward for both actor and critic.\n",
        "\n",
        "    Args:\n",
        "      x: the current state. It should contain 4 floats to represent the state.\n",
        "    \"\"\"\n",
        "    assert x.shape == (4,)\n",
        "    x = self.affine1(x)     # [H]\n",
        "    x = self.dropout(x)     # [H]\n",
        "    x = F.relu(x)\n",
        "\n",
        "    action_logits = self.actor_head(x)\n",
        "    action_prob = F.softmax(action_logits, dim=-1)   # [2]\n",
        "\n",
        "    state_value = self.value_head(x)                  # [1]\n",
        "\n",
        "    return action_prob, state_value"
      ],
      "metadata": {
        "id": "LODlVZkRnAc7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observer\n",
        "\n",
        "> Observer is in charge of interacting with the expensive env."
      ],
      "metadata": {
        "id": "ZxA45x20Zbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed.rpc as rpc\n",
        "\n",
        "class Observer:\n",
        "  \"\"\"An Observer is in charge of interacting with env.\n",
        "\n",
        "  In some RL setup, it is expensive to interact with the env, while it is\n",
        "  relatively cheap to run the agent (e.g. because it has small model). In this\n",
        "  case, we want to run multiple Observer in a distributed way, and reuse the\n",
        "  same agent.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, seed: int):\n",
        "    self.id = rpc.get_worker_info().id\n",
        "    self.env = gymnasium.make('CartPole-v1')\n",
        "    self.env.reset(seed=seed)\n",
        "\n",
        "  def run_episode(self, agent_rref):\n",
        "    \"\"\"Run a single episode (full trajectory).\"\"\"\n",
        "    state, _ = self.env.reset()\n",
        "    ep_reward = 0           # Total reward on this episode\n",
        "\n",
        "    for _ in range(10000):\n",
        "      # Send the state to the remote agent and get the action back.\n",
        "      action = agent_rref.rpc_sync().select_action(self.id, state)\n",
        "\n",
        "      state, reward, done, _, _ = self.env.step(action)\n",
        "\n",
        "      # Send the reward to the remote agent.\n",
        "      agent_rref.rpc_sync().report_reward(self.id, reward)\n",
        "\n",
        "      ep_reward += reward\n",
        "\n",
        "      if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "xdcjM5LjWS7U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent\n",
        "\n",
        "> Agent is in charge of predict the next action."
      ],
      "metadata": {
        "id": "oXgUhUS9ZkLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.rpc import RRef, rpc_async, remote\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, world_size):\n",
        "    self.ob_rrefs = []\n",
        "    self.agent_rref = RRef(self)\n",
        "    self.rewards = {}\n",
        "    self.saved_log_probs = {}\n",
        "    self.policy = Policy()\n",
        "    self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
        "    self.eps = np.finfo(np.float32).eps.item()\n",
        "    self.running_reward = 0     # The running epoch-level total reward.\n",
        "    self.reward_threshold = gymnasium.make('CartPole-v1').spec.reward_threshold\n",
        "\n",
        "    for ob_rank in range(world_size):\n",
        "      ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n",
        "      self.ob_rrefs.append(remote(ob_info, Observer))\n",
        "      self.rewards[ob_info.id] = []\n",
        "      self.saved_log_probs[ob_info.id] = []\n",
        "\n",
        "  def select_action(self, ob_id, state):\n",
        "    assert len(self.rewards[ob_id]) == len(self.saved_log_probs[ob_id])\n",
        "\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    action_prob, state_value = self.policy(state)\n",
        "\n",
        "    # Sample from the probs and emit the enum int of the next action\n",
        "    sampler = Categorical(action_prob)\n",
        "    next_action = sampler.sample()               # [1]\n",
        "\n",
        "    # save the action to buffer\n",
        "    #\n",
        "    # log_prob is the log of the probability of the selected action.\n",
        "    log_prob = sampler.log_prob(next_action)     # [1]\n",
        "    assert torch.allclose(log_prob, action_prob[next_action].log(), atol=1e-5)\n",
        "    self.saved_log_probs[ob_id].append(log_prob)\n",
        "\n",
        "    return next_action.item()\n",
        "\n",
        "  def report_reward(self, ob_id, reward):\n",
        "    self.rewards[ob_id].append(reward)\n",
        "\n",
        "    assert len(self.rewards[ob_id]) == len(self.saved_log_probs[ob_id])\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"Run one episode on each observer.\"\"\"\n",
        "    futs = []\n",
        "    for ob_rref in self.ob_rrefs:\n",
        "      handle = rpc_async(ob_rref.owner(), ob_rref.rpc_sync().run_episode, args=(self.agent_rref,))\n",
        "      futs.append(handle)\n",
        "\n",
        "    for fut in futs:\n",
        "      fut.wait()\n",
        "\n",
        "  def finish_episode(self):\n",
        "    # joins probs and rewards from different observers into lists\n",
        "    R, probs, rewards = 0, [], []\n",
        "    for ob_id in self.rewards:\n",
        "        probs.extend(self.saved_log_probs[ob_id])\n",
        "        rewards.extend(self.rewards[ob_id])\n",
        "\n",
        "    # use the minimum observer reward to calculate the running reward\n",
        "    min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n",
        "    self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n",
        "\n",
        "    # clear saved probs and rewards\n",
        "    for ob_id in self.rewards:\n",
        "        self.rewards[ob_id] = []\n",
        "        self.saved_log_probs[ob_id] = []\n",
        "\n",
        "    policy_loss, returns = [], []\n",
        "    for r in rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
        "    for log_prob, R in zip(probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    self.optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return min_reward"
      ],
      "metadata": {
        "id": "ilpzKnYiZson"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "def init_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  if rank == 0:\n",
        "    # rank 0 is the agent\n",
        "    # rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n",
        "    rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions(init_method=\"file://content/agent_share\"))\n",
        "    print(\"Agent RPC started!!!\")\n",
        "\n",
        "    agent = Agent(world_size)\n",
        "    print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n",
        "                \" is reached. Ctrl+C to exit.\")\n",
        "    for i_episode in count(1):\n",
        "      agent.run_episode()\n",
        "      last_reward = agent.finish_episode()\n",
        "\n",
        "      if i_episode % log_interval == 0:\n",
        "        print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n",
        "            f\"{agent.running_reward:.2f}\")\n",
        "      if agent.running_reward > agent.reward_threshold:\n",
        "        print(f\"Solved! Running reward is now {agent.running_reward}!\")\n",
        "        break\n",
        "    else:\n",
        "      # other ranks are the observer\n",
        "      # rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n",
        "      rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions(init_method=f\"file://content/ob_rank_{rank}\"))\n",
        "      print(f\"Observer RPC started on {rank=}!!!\")\n",
        "\n",
        "    # block until all rpcs finish, and shutdown the RPC instance\n",
        "    rpc.shutdown()\n",
        "\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29502' # You can choose a different port if 12355 is in use\n",
        "\n",
        "\n",
        "world_size = 4\n",
        "\n",
        "processes = []\n",
        "for rank in range(world_size):\n",
        "  p = mp.Process(target=init_process, args=(rank, world_size))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gUTG3KvyRVH",
        "outputId": "f92d659b-6274-474d-f726-26d67ee85156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4\n",
            "Starting process with rank=1, world_size=4\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n"
          ]
        }
      ]
    }
  ]
}