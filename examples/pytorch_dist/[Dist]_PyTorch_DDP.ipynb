{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL0RHIegC1C0XWLNM5EhtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/%5BDist%5D_PyTorch_DDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reference: https://zhuanlan.zhihu.com/p/178402798\n",
        "- Good read about the mechanism:\n",
        "  - https://zhuanlan.zhihu.com/p/187610959\n",
        "  - https://zhuanlan.zhihu.com/p/250471767\n",
        "  - https://zhuanlan.zhihu.com/p/485208899"
      ],
      "metadata": {
        "id": "1uxUUCXZ9_T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j_CMrufIC88d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single GPU"
      ],
      "metadata": {
        "id": "U6XXpSa1FRZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Init\n",
        "input = torch.randn(20, 10).to(DEVICE) # (20, 10)\n",
        "labels = torch.randn(20, 10).to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "model = torch.nn.Linear(10, 10).to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for it in range(1):\n",
        "  # forward\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(input)\n",
        "\n",
        "  # backward\n",
        "  loss_fn(outputs, labels).backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # check model params\n",
        "  print(f\"In epoch {it}\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f\"{name=}, {param.data=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g90S9hU6BQ2D",
        "outputId": "af713742-df80-413b-8957-946ece67fcca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0\n",
            "name='weight', param.data=tensor([[ 0.0944, -0.1266, -0.1251,  0.1248, -0.0659, -0.0834,  0.1643, -0.1748,\n",
            "          0.1602,  0.2234],\n",
            "        [-0.1549, -0.2573,  0.0233, -0.1648,  0.1694, -0.1869, -0.2008, -0.0400,\n",
            "         -0.0904, -0.2047],\n",
            "        [ 0.1007,  0.1481,  0.0845,  0.1624,  0.0025,  0.2104, -0.2733,  0.2917,\n",
            "          0.1926, -0.1286],\n",
            "        [-0.0789, -0.1440,  0.2480,  0.0249,  0.0506,  0.2015, -0.1588,  0.2132,\n",
            "         -0.2207, -0.1388],\n",
            "        [-0.0530, -0.2940, -0.2188, -0.2619, -0.1186, -0.1148, -0.0541,  0.0963,\n",
            "         -0.0057, -0.1375],\n",
            "        [-0.2125,  0.2258, -0.2172, -0.1571, -0.1958,  0.2219,  0.2494,  0.0186,\n",
            "         -0.1865,  0.0155],\n",
            "        [ 0.0577,  0.0483,  0.0066, -0.1421, -0.0218,  0.0573, -0.0655,  0.0892,\n",
            "         -0.1105,  0.0901],\n",
            "        [-0.0250,  0.1317, -0.1487, -0.2025, -0.2007,  0.1900,  0.1915,  0.1950,\n",
            "          0.1023,  0.0138],\n",
            "        [-0.0253, -0.2263, -0.0107, -0.1052, -0.0309,  0.1138, -0.0093, -0.1307,\n",
            "          0.0476, -0.0651],\n",
            "        [-0.1008, -0.1295, -0.1835,  0.1902,  0.0472, -0.1306,  0.1623,  0.0707,\n",
            "          0.1605,  0.1529]])\n",
            "name='bias', param.data=tensor([ 0.2977,  0.0091, -0.0333, -0.1386,  0.2557,  0.1151, -0.0040,  0.0774,\n",
            "         0.2714,  0.1051])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDP\n",
        "\n",
        "- Here we use CPU instead of GPU to reduce the system requirement.\n"
      ],
      "metadata": {
        "id": "2McyiArZFVIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try 1 - Manually dist the training data.\n",
        "\n",
        "- We expect the trained model weights to be exactly the same as the single CPU scenario."
      ],
      "metadata": {
        "id": "dddZ7RyoRtNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "def run_single_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  split_data_size = 20 // 4\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  # Create the train set.\n",
        "  if rank == 0:\n",
        "    inputs = torch.randn(20, 10)\n",
        "    inputs_split_list = torch.split(inputs, split_data_size, dim=0)\n",
        "    inputs_split_list = list(inputs_split_list)\n",
        "    assert (20 // split_data_size) == len(inputs_split_list)\n",
        "\n",
        "    targets = torch.randn(20, 10)\n",
        "    targets_split_list = torch.split(targets, split_data_size, dim=0)\n",
        "    targets_split_list = list(targets_split_list)\n",
        "    assert (20 // split_data_size) == len(targets_split_list)\n",
        "  else:\n",
        "    inputs_split_list = None\n",
        "    targets_split_list = None\n",
        "\n",
        "  # Split the train set and send to the distributed workers.\n",
        "  inputs_split = torch.zeros((split_data_size, 10), dtype=torch.float32)\n",
        "  dist.scatter(inputs_split, inputs_split_list, src=0)\n",
        "  inputs_split.to(DEVICE)\n",
        "\n",
        "  targets_split = torch.zeros((split_data_size, 10), dtype=torch.float32)\n",
        "  dist.scatter(targets_split, targets_split_list, src=0)\n",
        "  targets_split.to(DEVICE)\n",
        "\n",
        "  # Init the model\n",
        "  model = torch.nn.Linear(10, 10).to(DEVICE)\n",
        "  ddp_model = DDP(model, device_ids=None)\n",
        "  loss_fn = torch.nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=1)\n",
        "\n",
        "  # forward\n",
        "  optimizer.zero_grad()\n",
        "  outputs = ddp_model(inputs_split)\n",
        "\n",
        "  # backward\n",
        "  loss_fn(outputs, targets_split).backward()\n",
        "\n",
        "  # check model params\n",
        "  # if rank == 0:\n",
        "  #   print(\"Before backward\")\n",
        "  #   for name, param in ddp_model.named_parameters():\n",
        "  #     if param.requires_grad:\n",
        "  #       print(f\"{name=}, {param.data=}\")\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # check model params\n",
        "  if rank == 0:\n",
        "    print(\"After backward\")\n",
        "    for name, param in ddp_model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        print(f\"{name=}, {param.data=}\")\n",
        "\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n",
        "\n",
        "world_size = 4\n",
        "\n",
        "processes = []\n",
        "for rank in range(world_size):\n",
        "  p = mp.Process(target=run_single_process, args=(rank, world_size))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAPDEnzOJMd5",
        "outputId": "78360a26-c549-41be-b7c6-d9fbe7a88b0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4\n",
            "Starting process with rank=1, world_size=4\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n",
            "After backward\n",
            "name='module.weight', param.data=tensor([[ 0.0944, -0.1266, -0.1251,  0.1248, -0.0659, -0.0834,  0.1643, -0.1748,\n",
            "          0.1602,  0.2234],\n",
            "        [-0.1549, -0.2573,  0.0233, -0.1648,  0.1694, -0.1869, -0.2008, -0.0400,\n",
            "         -0.0904, -0.2047],\n",
            "        [ 0.1007,  0.1481,  0.0845,  0.1624,  0.0025,  0.2104, -0.2733,  0.2917,\n",
            "          0.1926, -0.1286],\n",
            "        [-0.0789, -0.1440,  0.2480,  0.0249,  0.0506,  0.2015, -0.1588,  0.2132,\n",
            "         -0.2207, -0.1388],\n",
            "        [-0.0530, -0.2940, -0.2188, -0.2619, -0.1186, -0.1148, -0.0541,  0.0963,\n",
            "         -0.0057, -0.1375],\n",
            "        [-0.2125,  0.2258, -0.2172, -0.1571, -0.1958,  0.2219,  0.2494,  0.0186,\n",
            "         -0.1865,  0.0155],\n",
            "        [ 0.0577,  0.0483,  0.0066, -0.1421, -0.0218,  0.0573, -0.0655,  0.0892,\n",
            "         -0.1105,  0.0901],\n",
            "        [-0.0250,  0.1317, -0.1487, -0.2025, -0.2007,  0.1900,  0.1915,  0.1950,\n",
            "          0.1023,  0.0138],\n",
            "        [-0.0253, -0.2263, -0.0107, -0.1052, -0.0309,  0.1138, -0.0093, -0.1307,\n",
            "          0.0476, -0.0651],\n",
            "        [-0.1008, -0.1295, -0.1835,  0.1902,  0.0472, -0.1306,  0.1623,  0.0707,\n",
            "          0.1605,  0.1529]])\n",
            "name='module.bias', param.data=tensor([ 0.2977,  0.0091, -0.0333, -0.1386,  0.2557,  0.1151, -0.0040,  0.0774,\n",
            "         0.2714,  0.1051])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try 2 - use distributed sampler"
      ],
      "metadata": {
        "id": "3awzclzMUJ8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "fOEw2jMcFWMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "# This is the global bs. In DPP, it should guarantee the sum of bs on all devices equal to this #.\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "WORLD_SIZE = 4\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(ToyModel, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "      self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = x.view(-1, 16 * 5 * 5)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "# from the official doc: https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "      self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "dataset_transform = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])"
      ],
      "metadata": {
        "id": "ljM4jc72bxCl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title single CPU version\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# Init the model\n",
        "model = ToyModel().to(DEVICE)\n",
        "model.train()\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Dataset\n",
        "download_path = \"./data\"\n",
        "my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n",
        "trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for it, (data, label) in enumerate(trainloader):\n",
        "\n",
        "    # forward\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "\n",
        "    # backward\n",
        "    loss = loss_fn(outputs, label)\n",
        "    loss.backward()\n",
        "\n",
        "    if it % 200 == 0:\n",
        "      print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9X0kAQlbsLS",
        "outputId": "71dafbd9-bf17-40a6-8908-c642a56a39eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, it=0, loss=2.316\n",
            "epoch=0, it=200, loss=2.283\n",
            "epoch=0, it=400, loss=2.300\n",
            "epoch=0, it=600, loss=2.299\n",
            "epoch=0, it=800, loss=2.305\n",
            "epoch=0, it=1000, loss=2.327\n",
            "epoch=0, it=1200, loss=2.172\n",
            "epoch=0, it=1400, loss=2.208\n",
            "epoch=0, it=1600, loss=2.459\n",
            "epoch=0, it=1800, loss=1.976\n",
            "epoch=0, it=2000, loss=2.019\n",
            "epoch=0, it=2200, loss=2.011\n",
            "epoch=0, it=2400, loss=1.939\n",
            "epoch=0, it=2600, loss=1.686\n",
            "epoch=0, it=2800, loss=1.692\n",
            "epoch=0, it=3000, loss=2.043\n",
            "epoch=1, it=0, loss=1.656\n",
            "epoch=1, it=200, loss=2.215\n",
            "epoch=1, it=400, loss=1.709\n",
            "epoch=1, it=600, loss=1.913\n",
            "epoch=1, it=800, loss=1.589\n",
            "epoch=1, it=1000, loss=1.826\n",
            "epoch=1, it=1200, loss=1.525\n",
            "epoch=1, it=1400, loss=1.739\n",
            "epoch=1, it=1600, loss=1.653\n",
            "epoch=1, it=1800, loss=1.533\n",
            "epoch=1, it=2000, loss=1.563\n",
            "epoch=1, it=2200, loss=1.679\n",
            "epoch=1, it=2400, loss=1.578\n",
            "epoch=1, it=2600, loss=1.317\n",
            "epoch=1, it=2800, loss=1.551\n",
            "epoch=1, it=3000, loss=1.973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dist version\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "def run_single_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=WORLD_SIZE, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  # Init the model\n",
        "  # 模型是先注册，再同步的。\n",
        "  # 先定义一个 普通的model(nn.module) 然后再DDP(model)。\n",
        "  # 第二个操作会把master节点(rank 0)的model的parameter和buffer给同步出去。\n",
        "  #\n",
        "  # DDP初始化（也就是model = DDP(model)这一步）\n",
        "  # 1. 把parameter，buffer从master节点传到其他节点，使所有进程上的状态一致。\n",
        "  #   注释：DDP通过这一步保证所有进程的初始状态一致。所以，请确保在这一步之后，你的代码不会再修改模型的任何东西了，包括添加、修改、删除parameter和buffer！\n",
        "  # 2.（可能）如果有每个节点有多卡，则在每张卡上创建模型（类似DP）\n",
        "  # 3. 把parameter进行分组，每一组称为一个bucket。临近的parameter在同一个bucket。\n",
        "  #   注释：这是为了加速，在梯度通讯时，先计算、得到梯度的bucket会马上进行通讯，不必等到所有梯度计算结束才进行通讯。后面会详细介绍。\n",
        "  # 4. 创建管理器reducer，给每个parameter注册梯度平均的hook。\n",
        "  #   注释：这一步的具体实现是在C++代码里面的，即reducer.h文件。\n",
        "  # 5.（可能）为可能的SyncBN层做准备\n",
        "  #\n",
        "  # 在每个step中，DDP模型都会做下面的事情：\n",
        "  # 1. 采样数据，从dataloader得到一个batch的数据，用于当前计算（for data, label in dataloader）。\n",
        "  #   注释：因为我们的dataloader使用了DistributedSampler，所以各个进程之间的数据是不会重复的。如果要确保DDP性能和单卡性能一致，这边需要保证在数据上，DDP模式下的一个epoch和单卡下的一个epoch是等效的。\n",
        "  # 2. 进行网络的前向计算（prediction = model(data)）\n",
        "  #   2.1 同步各进程状态\n",
        "  #     2.1.1（可能）对单进程多卡复制模式，要在进程内同步多卡之间的parameter和buffer\n",
        "  #     2.1.2 同步各进程之间的buffer。\n",
        "  #   2.2 接下来才是进行真正的前向计算\n",
        "  #   2.3（可能）当DDP参数find_unused_parameter为true时，其会在forward结束时，启动一个回溯，标记出所有没被用到的parameter，提前把这些设定为ready。\n",
        "  #     注释：find_unused_parameter的默认值是false，因为其会拖慢速度。\n",
        "  # 3. 计算梯度（loss.backward()）\n",
        "  #   3.1 reducer外面：各个进程各自开始反向地计算梯度。\n",
        "  #     3.1.1 注释：梯度是反向计算的，所以最后面的参数反而是最先得到梯度的。\n",
        "  #   3.2 reducer外面：当某个parameter的梯度计算好了的时候，其之前注册的grad hook就会被触发，在reducer里把这个parameter的状态标记为ready。\n",
        "  #   3.3 reducer里面：当某个bucket的所有parameter都是ready状态时，reducer会开始对这个bucket的所有parameter都开始一个异步的all-reduce梯度平均操作。\n",
        "  #     注释：\n",
        "  #       3.3.1 bucket的执行过程也是有顺序的，其顺序与parameter是相反的，即最先注册的parameter的bucket在最后面。\n",
        "  #       3.3.2 所以，我们在创建module的时候，请务必把先进行计算的parameter注册在前面，后计算的在后面。不然，reducer会卡在某一个bucket等待，使训练时间延长！\n",
        "  #         3.3.2.1 所谓的参数注册，其实就是创建网络层。也就是要求按照网络计算顺序，依次创建网络层。\n",
        "  #   3.4 reducer里面：当所有bucket的梯度平均都结束后，reducer才会把得到的平均grad结果正式写入到parameter.grad里面。\n",
        "  #   注释：这一步，感觉没有必要等全部结束之后才进行。可能得对照一下源码。\n",
        "  # 4. 优化器optimizer应用gradient，更新参数（optimizer.step()）。\n",
        "  #   注释：这一步，是和DDP没关系的。\n",
        "  model = ToyModel().to(DEVICE)\n",
        "  ddp_model = DDP(model, device_ids=None)\n",
        "  ddp_model.train()\n",
        "  loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "  # Init the optimizer.\n",
        "  #\n",
        "  # 我们可以看到，因为optimizer和DDP是没有关系的，所以optimizer初始状态的同一性是不被DDP保证的！\n",
        "  # 大多数官方optimizer，其实现能保证从同样状态的model初始化时，其初始状态是相同的。\n",
        "  # 所以这边我们只要保证在DDP模型创建后才初始化optimizer，就不用做额外的操作。\n",
        "  # 但是，如果自定义optimizer，则需要你自己来保证其统一性！\n",
        "  # 回顾一下文章最开始的代码，你会发现，optimizer确实是在DDP之后定义的。这个时候的模式已经是被初始化为相同的参数，所以能够保证优化器的初始状态是相同的。\n",
        "  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Dataset\n",
        "  download_path = f\"./data_{rank}\"\n",
        "  my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n",
        "  # DDP：使用DistributedSampler，DDP帮我们把细节都封装起来了。\n",
        "  #      用，就完事儿！\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset)\n",
        "  # DDP：需要注意的是，这里的batch_size指的是每个进程下的batch_size。\n",
        "  #      也就是说，总batch_size是这里的batch_size再乘以并行数(world_size)。\n",
        "  assert BATCH_SIZE % WORLD_SIZE == 0\n",
        "  trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE//WORLD_SIZE, sampler=train_sampler)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    # The distributed training loss is not going to be the same as the single device training.\n",
        "    # The reason is that the distributed sampler uses \"epoch\" as the sampling seed in each host.\n",
        "    #\n",
        "    # 不知道你有没有好奇，为什么给dataloader加一个DistributedSampler，就可以无缝对接DDP模式呢？\n",
        "    # 其实原理很简单，就是给不同进程分配数据集的不重叠、不交叉部分。\n",
        "    # 那么问题来了，每次epoch我们都会随机shuffle数据集，那么，不同进程之间要怎么保持shuffle后数据集的一致性呢？\n",
        "    # DistributedSampler的实现方式是，不同进程会使用一个相同的随机数种子，这样shuffle出来的东西就能确保一致。\n",
        "    #\n",
        "    # 具体实现上，DistributedSampler使用当前epoch作为随机数种子，从而使得不同epoch下有不同的shuffle结果。\n",
        "    # 所以，记得每次epoch开始前都要调用一下sampler的set_epoch方法，这样才能让数据集随机shuffle起来。\n",
        "    trainloader.sampler.set_epoch(epoch)\n",
        "\n",
        "    for it, (data, label) in enumerate(trainloader):\n",
        "\n",
        "      # forward\n",
        "      optimizer.zero_grad()\n",
        "      outputs = ddp_model(data)\n",
        "\n",
        "      # backward\n",
        "      loss = loss_fn(outputs, label)\n",
        "      # The updated gradients are communicated to all workers during backward()\n",
        "      # It divides the models and params to buckets, and eagerly to communicate\n",
        "      # the buckets whose gradient calculation are finished, to reduce latency.\n",
        "      #\n",
        "      # see more in https://zhuanlan.zhihu.com/p/485208899\n",
        "      loss.backward()\n",
        "\n",
        "      if rank == 0 and it % 200 == 0:\n",
        "        print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n",
        "\n",
        "processes = []\n",
        "for rank in range(WORLD_SIZE):\n",
        "  p = mp.Process(target=run_single_process, args=(rank, WORLD_SIZE))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez5NWWEDUQLj",
        "outputId": "f26e1fa1-e424-4412-e0ca-323d512f80da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4\n",
            "Starting process with rank=1, world_size=4\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 24.5MB/s]\n",
            "100%|██████████| 170M/170M [00:06<00:00, 24.5MB/s]\n",
            "100%|██████████| 170M/170M [00:07<00:00, 24.3MB/s]\n",
            "100%|██████████| 170M/170M [00:07<00:00, 24.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, it=0, loss=2.284\n",
            "epoch=0, it=200, loss=2.244\n",
            "epoch=0, it=400, loss=2.296\n",
            "epoch=0, it=600, loss=2.322\n",
            "epoch=0, it=800, loss=2.286\n",
            "epoch=0, it=1000, loss=2.185\n",
            "epoch=0, it=1200, loss=2.227\n",
            "epoch=0, it=1400, loss=1.905\n",
            "epoch=0, it=1600, loss=2.099\n",
            "epoch=0, it=1800, loss=1.635\n",
            "epoch=0, it=2000, loss=2.344\n",
            "epoch=0, it=2200, loss=1.547\n",
            "epoch=0, it=2400, loss=2.609\n",
            "epoch=0, it=2600, loss=2.036\n",
            "epoch=0, it=2800, loss=1.854\n",
            "epoch=0, it=3000, loss=1.633\n",
            "epoch=1, it=0, loss=1.158\n",
            "epoch=1, it=200, loss=1.877\n",
            "epoch=1, it=400, loss=2.356\n",
            "epoch=1, it=600, loss=1.611\n",
            "epoch=1, it=800, loss=1.564\n",
            "epoch=1, it=1000, loss=1.683\n",
            "epoch=1, it=1200, loss=0.823\n",
            "epoch=1, it=1400, loss=1.330\n",
            "epoch=1, it=1600, loss=1.527\n",
            "epoch=1, it=1800, loss=2.017\n",
            "epoch=1, it=2000, loss=1.623\n",
            "epoch=1, it=2200, loss=0.835\n",
            "epoch=1, it=2400, loss=1.886\n",
            "epoch=1, it=2600, loss=2.017\n",
            "epoch=1, it=2800, loss=1.238\n",
            "epoch=1, it=3000, loss=1.414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speed up the Batch Inference\n",
        "\n",
        "- Idea is to distribute the data in the batch inference to different GPUs\n",
        "- No need to use the **DDP** wrapper, because it helps only the back-prop\n",
        "\n",
        "解决问题的思路很简单，就是各个进程中各自进行单卡的inference，然后把结果收集到一起。单卡inference很简单，我们甚至可以直接用DDP包装前的模型。问题其实只有两个：\n",
        "- 我们要如何把数据split到各个进程中\n",
        "- 我们要如何把结果合并到一起\n",
        "\n",
        "如何把数据split到各个进程中：新的data sampler\n",
        "大家肯定还记得，在训练的时候，我们用的 torch.utils.data.distributed.DistributedSampler帮助我们把数据不重复地分到各个进程上去。但是，其分的方法是：每段连续的N个数据，拆成一个一个，分给N个进程，所以每个进程拿到的数据不是连续的。这样，*不利于我们在inference结束的时候将结果合并到一起*。\n",
        "\n",
        "所以，这里我们需要实现一个新的data sampler。它的功能，是能够连续地划分数据块，不重复地分到各个进程上去。"
      ],
      "metadata": {
        "id": "yISx9wUdEEsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The sampler that shard the data sequentially\n",
        "\n",
        "# This makes it easier to gather & combine the results.\n",
        "\n",
        "import math\n",
        "\n",
        "class SequentialDistributionSampler(torch.utils.data.sampler.Sampler):\n",
        "  pass\n",
        "\n",
        "  def __init__(self, dataset, batch_size, rank: int, num_replicas: int):\n",
        "    self.dataset = dataset\n",
        "    self.batch_size = batch_size\n",
        "    self.rank = rank\n",
        "    self.num_replicas = num_replicas\n",
        "    # The # of samples in a single node.\n",
        "    self.num_samples = math.ceil(len(self.dataset) / num_replicas)\n",
        "    self.total_size = self.num_samples * num_replicas\n",
        "\n",
        "  def __iter__(self):\n",
        "    indices = list(range(len(self.dataset)))\n",
        "    indices += [-1] * (self.total_size - len(indices))\n",
        "    indices = indices[rank * self.num_samples : (rank + 1) * self.num_samples]\n",
        "    return iter(indices)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_samples"
      ],
      "metadata": {
        "id": "yVV-DHKOE7bX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cpu'\n",
        "\n",
        "def eval_single_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=WORLD_SIZE, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  download_path = f\"./data_{rank}\"\n",
        "  my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n",
        "\n",
        "  sampler = SequentialDistributionSampler(dataset=my_trainset, batch_size=BATCH_SIZE, rank=rank, num_replicas=world_size)\n",
        "  evalloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE//WORLD_SIZE, sampler=sampler)\n",
        "\n",
        "  model = ToyModel().to(DEVICE)\n",
        "\n",
        "  results = None\n",
        "  for it, (data, label) in enumerate(evalloader):\n",
        "      # forward\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(data)\n",
        "\n",
        "      if results is not None:\n",
        "        results = torch.cat((results, outputs), dim = 0)\n",
        "      else:\n",
        "        results = outputs\n",
        "\n",
        "      if rank == 0 and it % 200 == 0:\n",
        "        print(f\"{it=}, num of results = {len(results) if results is not None else 0}\")\n",
        "\n",
        "      # break\n",
        "\n",
        "  if rank == 0:\n",
        "    gathered = [torch.zeros(results.shape, dtype=torch.float32) for _ in range(world_size)]\n",
        "  else:\n",
        "    gathered = None\n",
        "  dist.gather(results, gather_list=gathered, dst=0)\n",
        "\n",
        "  if rank == 0:\n",
        "    gathered_t = torch.cat(gathered, axis=0)\n",
        "    valid_gathered_t = gathered_t[:len(my_trainset)]\n",
        "    print(f\"Gathered eval results size = {gathered_t.shape}, valid results size = {valid_gathered_t.shape}\")\n",
        "\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n",
        "\n",
        "processes = []\n",
        "for rank in range(WORLD_SIZE):\n",
        "  p = mp.Process(target=eval_single_process, args=(rank, WORLD_SIZE))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmSD1N5fKsfn",
        "outputId": "2e95233f-ee89-4d3c-caff-b7385965c93e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4Starting process with rank=1, world_size=4\n",
            "\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n",
            "it=0, num of results = 4\n",
            "it=200, num of results = 804\n",
            "it=400, num of results = 1604\n",
            "it=600, num of results = 2404\n",
            "it=800, num of results = 3204\n",
            "it=1000, num of results = 4004\n",
            "it=1200, num of results = 4804\n",
            "it=1400, num of results = 5604\n",
            "it=1600, num of results = 6404\n",
            "it=1800, num of results = 7204\n",
            "it=2000, num of results = 8004\n",
            "it=2200, num of results = 8804\n",
            "it=2400, num of results = 9604\n",
            "it=2600, num of results = 10404\n",
            "it=2800, num of results = 11204\n",
            "it=3000, num of results = 12004\n",
            "Gathered eval results size = torch.Size([50000, 10]), valid results size = torch.Size([50000, 10])\n"
          ]
        }
      ]
    }
  ]
}