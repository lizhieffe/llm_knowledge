{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The colab assumes there are 2 GPUs on a single node.\n\nThis can run on Kaggle Notebook.","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:33:05.530882Z","iopub.execute_input":"2025-08-05T05:33:05.531586Z","iopub.status.idle":"2025-08-05T05:33:05.913781Z","shell.execute_reply.started":"2025-08-05T05:33:05.531549Z","shell.execute_reply":"2025-08-05T05:33:05.913062Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dist basics on GPUs","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Process for collective communication\n\n# 广播（Broadcast）：广播是一种将数据从一个源进程发送到所有其他进程的通信操作。在 torch.distributed 中，通过 broadcast(tensor, src=0) 可以实现该操作，将 rank 为 0 的进程中的数据广播到所有其他进程。广播操作能够确保所有进程拥有相同的数据，适合需要共享模型参数、初始化权重等场景。比如在分布式训练的初始化阶段，用于将主进程的模型参数广播到所有其他进程，保证训练从同样的初始参数开始。\n# 规约（Reduce 和 All-Reduce）：规约操作是一种将多个进程的数据进行计算（如求和、求最大值等）的操作。常用的规约操作有两种，reduce()：一个进程（通常是主进程）收集并合并来自所有进程的数据；all_reduce()：所有进程同时得到合并后的数据。比如 all_reduce(tensor, op=ReduceOp.SUM) 会在所有进程中求和，并将结果存放在每个进程的 tensor 中。规约操作能有效减少通信负担，适用于大规模梯度汇总或模型权重更新。譬如在分布式训练中，all_reduce 常用于梯度求和，以确保在多个进程中的梯度保持一致，实现同步更新。\n# 收集（Gather 和 All-Gather）：收集操作是将多个进程的数据收集到一个或多个进程的操作：gather()：将多个进程的数据收集到一个进程中。all_gather()：所有进程都收集到全部进程的数据。例如 all_gather(gathered_tensors, tensor) 会将所有进程中的 tensor 收集到每个进程的 gathered_tensors 列表中。收集操作方便对所有进程中的数据进行后续分析和处理。譬如做 evaluation 时，可以使用 all_gather 来汇总各个进程的中间结果。\n# 散发（Scatter）：scatter() 操作是将一个进程的数据分散到多个进程中。例如在 rank 为 0 的进程中有一个包含若干子张量的列表，scatter() 可以将列表中的每个子张量分配给其他进程。适用于数据分发，将大型数据集或模型权重在多个进程中分散，以便每个进程可以处理不同的数据块。\n\ndef init_process(rank, world_size, backend=\"nccl\"):\n  device = f\"cuda:{rank}\"\n  print(f\"Starting process with {rank=}, {world_size=} {device=}\")\n\n  # Use the gloo backend for CPU-based distributed processing\n  dist.init_process_group(backend=\"nccl\", world_size=world_size, rank=rank)\n\n  assert rank == dist.get_rank()\n  assert world_size == dist.get_world_size()\n  dist.barrier()\n  print(\"{rank=} Finished init!!!\")\n\n  # Task 1 - all gather\n  # It gathers information from all nodes.\n  if rank == 0:\n    print(\"\\nTask 1 - all gather\")\n  process_info = (\n      f\"Process {rank} Information...\"\n  )\n  max_len = 100\n  process_info_tensor = torch.zeros(max_len, dtype=torch.int32).to(device)\n  process_info_bytes = process_info.encode('utf-8')\n  process_info_tensor[:len(process_info_bytes)] = torch.tensor([b for b in process_info_bytes], dtype=torch.int32)\n\n  gathered_tensors = [torch.zeros(max_len, dtype=torch.int32).to(device) for _ in range(world_size)]\n\n  dist.all_gather(gathered_tensors, process_info_tensor)\n\n  if rank == 0:\n    for t in gathered_tensors:\n      info_bytes = t.to('cpu').numpy().astype('uint8').tobytes()\n      info_str = info_bytes.decode('utf-8', 'ignore').strip('\\x00')\n      print(info_str)\n  dist.barrier()\n  print(\"{rank=} Finished step 1!!!\")\n\n  # Task 2 - all reduce (sum)\n  if rank == 0:\n    print(\"\\nTask 2 - all reduce\")\n  tensor = torch.ones((4,)).to(device)\n  dist.all_reduce(tensor)\n  print(f\"All reduce for all processes: in rank {rank}, tensor = {tensor}\")\n  dist.barrier()\n\n  # Task 3 - all reduce (sum) in a sub-group.\n  if rank == 0:\n    print(\"\\nTask 3 - all reduce for sub-group\")\n  sub_group_ranks = range(1, world_size, 2)\n  sub_group = dist.new_group(ranks=sub_group_ranks)\n  if rank in sub_group_ranks:\n    sub_group_tensor = torch.ones((4,)).to(device)\n    dist.all_reduce(sub_group_tensor, group=sub_group)\n    print(f\"Sub group all reduce: in rank {rank}, tensor = {sub_group_tensor}\")\n  dist.barrier()\n\n  # Task 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\n  if rank == 0:\n    print(\"\\nRank 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\")\n  group_1_sum = torch.tensor([1, 1, 1, 1]).to(device)\n  group_2_sum = torch.tensor([1.5] * 4).to(device)\n  group_1_ranks = list(range(world_size // 2))\n  group_2_ranks = list(range(world_size // 2, world_size))\n  group_1 = dist.new_group(ranks=group_1_ranks)\n  group_2 = dist.new_group(ranks=group_2_ranks)\n  if rank in group_1_ranks:\n    dist.all_reduce(group_1_sum, group=group_1)\n  else:\n    dist.all_reduce(group_2_sum, group=group_2)\n  # Communicate the sub-group sums to the entire group.\n  dist.all_reduce(group_1_sum, op=dist.ReduceOp.MAX)\n  dist.all_reduce(group_2_sum, op=dist.ReduceOp.MAX)\n  print(f\"In rank {rank}, {group_1_sum.to('cpu')=}, {group_2_sum.to('cpu')=}\")\n\n  # Finish\n  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n  dist.destroy_process_group()\n\n# Run the distributed processing\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12459' # You can choose a different port if 12355 is in use\n\nworld_size = 2\n\nprocesses = []\nfor rank in range(world_size):\n  p = mp.Process(target=init_process, args=(rank, world_size))\n  p.start()\n  processes.append(p)\n\nfor p in processes:\n  p.join()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PyTorch DDP Library Usage\n\n* In this section, we demonstrate the usage of the PyTorch DDP library.","metadata":{}},{"cell_type":"markdown","source":"## Try 1 - Manually dist the training data\n\n- We expect the trained model weights to be exactly the same as the single CPU scenario.\n","metadata":{}},{"cell_type":"markdown","source":"Single-GPU version","metadata":{}},{"cell_type":"code","source":"import torch\n\nDEVICE = 'cuda:0'\n\ntorch.manual_seed(123)\n\n# Init\ninput = torch.randn(20, 10).to(DEVICE) # (20, 10)\nlabels = torch.randn(20, 10).to(DEVICE)\n\nloss_fn = torch.nn.MSELoss()\n\nmodel = torch.nn.Linear(10, 10).to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=1)\n\nfor it in range(1):\n  # forward\n  optimizer.zero_grad()\n  outputs = model(input)\n\n  # backward\n  loss_fn(outputs, labels).backward()\n  optimizer.step()\n\n  # check model params\n  print(f\"In epoch {it}\")\n  for name, param in model.named_parameters():\n    if param.requires_grad:\n      print(f\"{name=}, {param.data=}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Multi-GPU version","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\n\ndef run_single_process(rank: int, world_size: int):\n  DEVICE = f\"cuda:{rank}\"\n  print(f\"Starting process with {rank=}, {world_size=}, {DEVICE=}\")\n\n  # Use the NCCL backend for GPU-based distributed processing\n  dist.init_process_group(backend=\"nccl\", world_size=world_size, rank=rank)\n\n  assert rank == dist.get_rank()\n  assert world_size == dist.get_world_size()\n  dist.barrier()\n\n  split_data_size = 20 // world_size\n\n  torch.manual_seed(123)\n\n  # Create the train set.\n  if rank == 0:\n    inputs = torch.randn(20, 10).to(DEVICE)\n    inputs_split_list = torch.split(inputs, split_data_size, dim=0)\n    inputs_split_list = list(inputs_split_list)\n    assert (20 // split_data_size) == len(inputs_split_list)\n\n    targets = torch.randn(20, 10).to(DEVICE)\n    targets_split_list = torch.split(targets, split_data_size, dim=0)\n    targets_split_list = list(targets_split_list)\n    assert (20 // split_data_size) == len(targets_split_list)\n  else:\n    inputs_split_list = None\n    targets_split_list = None\n\n  # Split the train set and send to the distributed workers.\n  inputs_split = torch.zeros((split_data_size, 10), dtype=torch.float32).to(DEVICE)\n  dist.scatter(inputs_split, inputs_split_list, src=0)\n  inputs_split.to(DEVICE)\n\n  targets_split = torch.zeros((split_data_size, 10), dtype=torch.float32).to(DEVICE)\n  dist.scatter(targets_split, targets_split_list, src=0)\n  targets_split.to(DEVICE)\n\n  # Init the model\n  model = torch.nn.Linear(10, 10).to(DEVICE)\n  ddp_model = DDP(model, device_ids=None)\n  loss_fn = torch.nn.MSELoss()\n  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=1)\n\n  # forward\n  optimizer.zero_grad()\n  outputs = ddp_model(inputs_split)\n\n  # backward\n  loss_fn(outputs, targets_split).backward()\n\n  # check model params\n  # if rank == 0:\n  #   print(\"Before backward\")\n  #   for name, param in ddp_model.named_parameters():\n  #     if param.requires_grad:\n  #       print(f\"{name=}, {param.data=}\")\n\n  optimizer.step()\n\n  # check model params\n  if rank == 0:\n    print(\"After backward\")\n    for name, param in ddp_model.named_parameters():\n      if param.requires_grad:\n        print(f\"{name=}, {param.data=}\")\n\n  dist.destroy_process_group()\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n\nworld_size = 2\n\nprocesses = []\nfor rank in range(world_size):\n  p = mp.Process(target=run_single_process, args=(rank, world_size))\n  p.start()\n  processes.append(p)\n\nfor p in processes:\n  p.join()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Try 2 - use distributed sampler to load the training data","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torchvision\n\nEPOCHS = 20\n\n# This is the global bs. In DPP, it should guarantee the sum of bs on all devices equal to this #.\nBATCH_SIZE = 16\n\nWORLD_SIZE = 2\n\nclass ToyModel(nn.Module):\n  def __init__(self):\n      super(ToyModel, self).__init__()\n      self.conv1 = nn.Conv2d(3, 6, 5)\n      self.pool = nn.MaxPool2d(2, 2)\n      self.conv2 = nn.Conv2d(6, 16, 5)\n      self.fc1 = nn.Linear(16 * 5 * 5, 120)\n      self.fc2 = nn.Linear(120, 84)\n      self.fc3 = nn.Linear(84, 10)\n\n  def forward(self, x):\n      x = self.pool(F.relu(self.conv1(x)))\n      x = self.pool(F.relu(self.conv2(x)))\n      x = x.view(-1, 16 * 5 * 5)\n      x = F.relu(self.fc1(x))\n      x = F.relu(self.fc2(x))\n      x = self.fc3(x)\n      return x\n\n# # from the official doc: https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n# class ToyModel(nn.Module):\n#   def __init__(self):\n#       super().__init__()\n#       self.conv1 = nn.Conv2d(3, 6, 5)\n#       self.pool = nn.MaxPool2d(2, 2)\n#       self.conv2 = nn.Conv2d(6, 16, 5)\n#       self.fc1 = nn.Linear(16 * 5 * 5, 120)\n#       self.fc2 = nn.Linear(120, 84)\n#       self.fc3 = nn.Linear(84, 10)\n\n#   def forward(self, x):\n#       x = self.pool(F.relu(self.conv1(x)))\n#       x = self.pool(F.relu(self.conv2(x)))\n#       x = torch.flatten(x, 1) # flatten all dimensions except batch\n#       x = F.relu(self.fc1(x))\n#       x = F.relu(self.fc2(x))\n#       x = self.fc3(x)\n#       return x\n\ndataset_transform = torchvision.transforms.Compose([\n      torchvision.transforms.ToTensor(),\n      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n  ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:34:06.531250Z","iopub.execute_input":"2025-08-05T05:34:06.532139Z","iopub.status.idle":"2025-08-05T05:34:06.540077Z","shell.execute_reply.started":"2025-08-05T05:34:06.532096Z","shell.execute_reply":"2025-08-05T05:34:06.539197Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Multi GPUs version","metadata":{}},{"cell_type":"code","source":"# Otherwise the cell below will error out.\n# See https://github.com/huggingface/accelerate/issues/940#issuecomment-1364114196\n#\n# To un-initialize the cuda, restart the runtime and avoid running other cells that init cuda.\nassert not torch.cuda.is_initialized()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:34:09.586587Z","iopub.execute_input":"2025-08-05T05:34:09.587439Z","iopub.status.idle":"2025-08-05T05:34:09.591426Z","shell.execute_reply.started":"2025-08-05T05:34:09.587412Z","shell.execute_reply":"2025-08-05T05:34:09.590439Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.nn.parallel import DistributedDataParallel as DDP\n\ndef run_single_process(rank: int, world_size: int):\n  DEVICE = f\"cuda:{rank}\"\n    \n  print(f\"Starting process with {rank=}, {world_size=}, {DEVICE=}\")\n\n  # Use the NCCL backend for GPU-based distributed processing\n  dist.init_process_group(backend=\"nccl\", world_size=WORLD_SIZE, rank=rank)\n\n  assert rank == dist.get_rank()\n  assert world_size == dist.get_world_size()\n  # dist.barrier()\n\n  torch.manual_seed(123)\n\n  # Init the model\n  # 模型是先注册，再同步的。\n  # 先定义一个 普通的model(nn.module) 然后再DDP(model)。\n  # 第二个操作会把master节点(rank 0)的model的parameter和buffer给同步出去。\n  #\n  # DDP初始化（也就是model = DDP(model)这一步）\n  # 1. 把parameter，buffer从master节点传到其他节点，使所有进程上的状态一致。\n  #   注释：DDP通过这一步保证所有进程的初始状态一致。所以，请确保在这一步之后，你的代码不会再修改模型的任何东西了，包括添加、修改、删除parameter和buffer！\n  # 2.（可能）如果有每个节点有多卡，则在每张卡上创建模型（类似DP）\n  # 3. 把parameter进行分组，每一组称为一个bucket。临近的parameter在同一个bucket。\n  #   注释：这是为了加速，在梯度通讯时，先计算、得到梯度的bucket会马上进行通讯，不必等到所有梯度计算结束才进行通讯。后面会详细介绍。\n  # 4. 创建管理器reducer，给每个parameter注册梯度平均的hook。\n  #   注释：这一步的具体实现是在C++代码里面的，即reducer.h文件。\n  # 5.（可能）为可能的SyncBN层做准备\n  #\n  # 在每个step中，DDP模型都会做下面的事情：\n  # 1. 采样数据，从dataloader得到一个batch的数据，用于当前计算（for data, label in dataloader）。\n  #   注释：因为我们的dataloader使用了DistributedSampler，所以各个进程之间的数据是不会重复的。如果要确保DDP性能和单卡性能一致，这边需要保证在数据上，DDP模式下的一个epoch和单卡下的一个epoch是等效的。\n  # 2. 进行网络的前向计算（prediction = model(data)）\n  #   2.1 同步各进程状态\n  #     2.1.1（可能）对单进程多卡复制模式，要在进程内同步多卡之间的parameter和buffer\n  #     2.1.2 同步各进程之间的buffer。\n  #   2.2 接下来才是进行真正的前向计算\n  #   2.3（可能）当DDP参数find_unused_parameter为true时，其会在forward结束时，启动一个回溯，标记出所有没被用到的parameter，提前把这些设定为ready。\n  #     注释：find_unused_parameter的默认值是false，因为其会拖慢速度。\n  # 3. 计算梯度（loss.backward()）\n  #   3.1 reducer外面：各个进程各自开始反向地计算梯度。\n  #     3.1.1 注释：梯度是反向计算的，所以最后面的参数反而是最先得到梯度的。\n  #   3.2 reducer外面：当某个parameter的梯度计算好了的时候，其之前注册的grad hook就会被触发，在reducer里把这个parameter的状态标记为ready。\n  #   3.3 reducer里面：当某个bucket的所有parameter都是ready状态时，reducer会开始对这个bucket的所有parameter都开始一个异步的all-reduce梯度平均操作。\n  #     注释：\n  #       3.3.1 bucket的执行过程也是有顺序的，其顺序与parameter是相反的，即最先注册的parameter的bucket在最后面。\n  #       3.3.2 所以，我们在创建module的时候，请务必把先进行计算的parameter注册在前面，后计算的在后面。不然，reducer会卡在某一个bucket等待，使训练时间延长！\n  #         3.3.2.1 所谓的参数注册，其实就是创建网络层。也就是要求按照网络计算顺序，依次创建网络层。\n  #   3.4 reducer里面：当所有bucket的梯度平均都结束后，reducer才会把得到的平均grad结果正式写入到parameter.grad里面。\n  #   注释：这一步，感觉没有必要等全部结束之后才进行。可能得对照一下源码。\n  # 4. 优化器optimizer应用gradient，更新参数（optimizer.step()）。\n  #   注释：这一步，是和DDP没关系的。\n  # model = ToyModel().to(DEVICE)\n  model = ToyModel()\n  model = model.to(DEVICE)\n  ddp_model = DDP(model, device_ids=None)\n  ddp_model.train()\n  loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n  print(\"The model is initiated!!!\")\n\n  # Init the optimizer.\n  #\n  # 我们可以看到，因为optimizer和DDP是没有关系的，所以optimizer初始状态的同一性是不被DDP保证的！\n  # 大多数官方optimizer，其实现能保证从同样状态的model初始化时，其初始状态是相同的。\n  # 所以这边我们只要保证在DDP模型创建后才初始化optimizer，就不用做额外的操作。\n  # 但是，如果自定义optimizer，则需要你自己来保证其统一性！\n  # 回顾一下文章最开始的代码，你会发现，optimizer确实是在DDP之后定义的。这个时候的模式已经是被初始化为相同的参数，所以能够保证优化器的初始状态是相同的。\n  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001, momentum=0.9)\n\n  # Dataset\n  download_path = f\"./data_{rank}\"\n  my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n  # DDP：使用DistributedSampler，DDP帮我们把细节都封装起来了。\n  #      用，就完事儿！\n  train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset)\n  # DDP：需要注意的是，这里的batch_size指的是每个进程下的batch_size。\n  #      也就是说，总batch_size是这里的batch_size再乘以并行数(world_size)。\n  assert BATCH_SIZE % WORLD_SIZE == 0\n  trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE//WORLD_SIZE, sampler=train_sampler)\n\n  for epoch in range(EPOCHS):\n    # The distributed training loss is not going to be the same as the single device training.\n    # The reason is that the distributed sampler uses \"epoch\" as the sampling seed in each host.\n    #\n    # 不知道你有没有好奇，为什么给dataloader加一个DistributedSampler，就可以无缝对接DDP模式呢？\n    # 其实原理很简单，就是给不同进程分配数据集的不重叠、不交叉部分。\n    # 那么问题来了，每次epoch我们都会随机shuffle数据集，那么，不同进程之间要怎么保持shuffle后数据集的一致性呢？\n    # DistributedSampler的实现方式是，不同进程会使用一个相同的随机数种子，这样shuffle出来的东西就能确保一致。\n    #\n    # 具体实现上，DistributedSampler使用当前epoch作为随机数种子，从而使得不同epoch下有不同的shuffle结果。\n    # 所以，记得每次epoch开始前都要调用一下sampler的set_epoch方法，这样才能让数据集随机shuffle起来。\n    trainloader.sampler.set_epoch(epoch)\n\n    for it, (data, label) in enumerate(trainloader):\n      data = data.to(DEVICE)\n      label = label.to(DEVICE)\n\n      # forward\n      optimizer.zero_grad()\n      outputs = ddp_model(data)\n\n      # backward\n      loss = loss_fn(outputs, label)\n      # The updated gradients are communicated to all workers during backward()\n      # It divides the models and params to buckets, and eagerly to communicate\n      # the buckets whose gradient calculation are finished, to reduce latency.\n      #\n      # see more in https://zhuanlan.zhihu.com/p/485208899\n      loss.backward()\n\n      if rank == 0 and it % 1000 == 0:\n        print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n\n      optimizer.step()\n\n  dist.destroy_process_group()\n\n\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12365' # You can choose a different port if 12355 is in use\n\nprocesses = []\nfor rank in range(WORLD_SIZE):\n  p = mp.Process(target=run_single_process, args=(rank, WORLD_SIZE))\n  p.start()\n  processes.append(p)\n\nfor p in processes:\n  p.join()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:34:11.295144Z","iopub.execute_input":"2025-08-05T05:34:11.295989Z","iopub.status.idle":"2025-08-05T05:40:13.102876Z","shell.execute_reply.started":"2025-08-05T05:34:11.295961Z","shell.execute_reply":"2025-08-05T05:40:13.101544Z"}},"outputs":[{"name":"stdout","text":"Starting process with rank=0, world_size=2, DEVICE='cuda:0'\nStarting process with rank=1, world_size=2, DEVICE='cuda:1'\nThe model is initiated!!!\nThe model is initiated!!!\nepoch=0, it=0, loss=2.284\nepoch=0, it=1000, loss=2.184\nepoch=0, it=2000, loss=2.077\nepoch=0, it=3000, loss=1.255\nepoch=1, it=0, loss=1.566\nepoch=1, it=1000, loss=1.656\nepoch=1, it=2000, loss=1.148\nepoch=1, it=3000, loss=1.347\nepoch=2, it=0, loss=1.631\nepoch=2, it=1000, loss=1.296\nepoch=2, it=2000, loss=1.237\nepoch=2, it=3000, loss=1.030\nepoch=3, it=0, loss=1.496\nepoch=3, it=1000, loss=1.020\nepoch=3, it=2000, loss=1.460\nepoch=3, it=3000, loss=0.716\nepoch=4, it=0, loss=0.923\nepoch=4, it=1000, loss=0.725\nepoch=4, it=2000, loss=0.722\nepoch=4, it=3000, loss=2.054\nepoch=5, it=0, loss=2.350\nepoch=5, it=1000, loss=1.431\nepoch=5, it=2000, loss=0.511\nepoch=5, it=3000, loss=0.348\nepoch=6, it=0, loss=0.767\nepoch=6, it=1000, loss=1.566\nepoch=6, it=2000, loss=0.967\nepoch=6, it=3000, loss=1.096\nepoch=7, it=0, loss=0.919\nepoch=7, it=1000, loss=0.957\nepoch=7, it=2000, loss=1.417\nepoch=7, it=3000, loss=0.706\nepoch=8, it=0, loss=0.915\nepoch=8, it=1000, loss=1.043\nepoch=8, it=2000, loss=0.585\nepoch=8, it=3000, loss=1.075\nepoch=9, it=0, loss=1.508\nepoch=9, it=1000, loss=0.803\nepoch=9, it=2000, loss=0.088\nepoch=9, it=3000, loss=0.800\nepoch=10, it=0, loss=0.867\nepoch=10, it=1000, loss=0.558\nepoch=10, it=2000, loss=1.662\nepoch=10, it=3000, loss=0.414\nepoch=11, it=0, loss=0.920\nepoch=11, it=1000, loss=0.504\nepoch=11, it=2000, loss=0.904\nepoch=11, it=3000, loss=1.378\nepoch=12, it=0, loss=0.606\nepoch=12, it=1000, loss=0.511\nepoch=12, it=2000, loss=1.481\nepoch=12, it=3000, loss=1.250\nepoch=13, it=0, loss=1.307\nepoch=13, it=1000, loss=1.218\nepoch=13, it=2000, loss=1.234\nepoch=13, it=3000, loss=0.931\nepoch=14, it=0, loss=0.488\nepoch=14, it=1000, loss=0.666\nepoch=14, it=2000, loss=0.794\nepoch=14, it=3000, loss=0.765\nepoch=15, it=0, loss=0.645\nepoch=15, it=1000, loss=0.718\nepoch=15, it=2000, loss=0.371\nepoch=15, it=3000, loss=1.424\nepoch=16, it=0, loss=0.609\nepoch=16, it=1000, loss=0.344\nepoch=16, it=2000, loss=0.358\nepoch=16, it=3000, loss=0.900\nepoch=17, it=0, loss=0.345\nepoch=17, it=1000, loss=0.369\nepoch=17, it=2000, loss=0.380\nepoch=17, it=3000, loss=0.379\nepoch=18, it=0, loss=0.519\nepoch=18, it=1000, loss=0.487\nepoch=18, it=2000, loss=0.367\nepoch=18, it=3000, loss=0.714\nepoch=19, it=0, loss=0.454\nepoch=19, it=1000, loss=0.733\nepoch=19, it=2000, loss=1.542\nepoch=19, it=3000, loss=0.663\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Single GPU version","metadata":{}},{"cell_type":"code","source":"from torch.nn.parallel import DistributedDataParallel as DDP\n\nDEVICE = 'cuda:0'\n\n# Init the model\nmodel = ToyModel().to(DEVICE)\nmodel.train()\nloss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Dataset\ndownload_path = \"./data\"\nmy_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\ntrainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE)\n\nfor epoch in range(EPOCHS):\n  for it, (data, label) in enumerate(trainloader):\n\n    data = data.to(DEVICE)\n    label = label.to(DEVICE)\n    # forward\n    optimizer.zero_grad()\n    outputs = model(data)\n\n    # backward\n    loss = loss_fn(outputs, label)\n    loss.backward()\n\n    if it % 1000 == 0:\n      print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:40:31.829729Z","iopub.execute_input":"2025-08-05T05:40:31.830402Z"}},"outputs":[{"name":"stdout","text":"epoch=0, it=0, loss=2.295\nepoch=0, it=1000, loss=2.300\nepoch=0, it=2000, loss=2.129\nepoch=0, it=3000, loss=1.813\nepoch=1, it=0, loss=1.598\nepoch=1, it=1000, loss=1.659\nepoch=1, it=2000, loss=1.492\nepoch=1, it=3000, loss=1.881\nepoch=2, it=0, loss=1.255\nepoch=2, it=1000, loss=1.708\nepoch=2, it=2000, loss=1.389\nepoch=2, it=3000, loss=1.752\nepoch=3, it=0, loss=1.004\nepoch=3, it=1000, loss=1.540\nepoch=3, it=2000, loss=1.190\nepoch=3, it=3000, loss=1.657\nepoch=4, it=0, loss=0.866\nepoch=4, it=1000, loss=1.366\nepoch=4, it=2000, loss=0.996\nepoch=4, it=3000, loss=1.607\nepoch=5, it=0, loss=0.778\nepoch=5, it=1000, loss=1.309\nepoch=5, it=2000, loss=0.907\nepoch=5, it=3000, loss=1.496\nepoch=6, it=0, loss=0.729\nepoch=6, it=1000, loss=1.210\nepoch=6, it=2000, loss=0.931\nepoch=6, it=3000, loss=1.399\nepoch=7, it=0, loss=0.721\nepoch=7, it=1000, loss=1.168\nepoch=7, it=2000, loss=0.902\nepoch=7, it=3000, loss=1.318\nepoch=8, it=0, loss=0.663\nepoch=8, it=1000, loss=1.116\nepoch=8, it=2000, loss=0.879\nepoch=8, it=3000, loss=1.292\nepoch=9, it=0, loss=0.588\nepoch=9, it=1000, loss=1.090\nepoch=9, it=2000, loss=0.827\nepoch=9, it=3000, loss=1.268\nepoch=10, it=0, loss=0.539\n","output_type":"stream"}],"execution_count":null}]}