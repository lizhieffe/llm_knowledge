{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bc3445",
   "metadata": {
    "papermill": {
     "duration": 0.002237,
     "end_time": "2025-08-05T03:53:49.009860",
     "exception": false,
     "start_time": "2025-08-05T03:53:49.007623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The colab assumes there are 2 GPUs on a single node.\n",
    "\n",
    "This can run on Kaggle Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8302349d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-05T03:53:49.014317Z",
     "iopub.status.busy": "2025-08-05T03:53:49.014041Z",
     "iopub.status.idle": "2025-08-05T03:53:50.866123Z",
     "shell.execute_reply": "2025-08-05T03:53:50.865197Z"
    },
    "papermill": {
     "duration": 1.856174,
     "end_time": "2025-08-05T03:53:50.867693",
     "exception": false,
     "start_time": "2025-08-05T03:53:49.011519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b498e794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T03:53:50.871964Z",
     "iopub.status.busy": "2025-08-05T03:53:50.871635Z",
     "iopub.status.idle": "2025-08-05T03:53:51.102509Z",
     "shell.execute_reply": "2025-08-05T03:53:51.101805Z"
    },
    "papermill": {
     "duration": 0.234375,
     "end_time": "2025-08-05T03:53:51.103846",
     "exception": false,
     "start_time": "2025-08-05T03:53:50.869471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  5 03:53:50 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   43C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   44C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee70936d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T03:53:51.108873Z",
     "iopub.status.busy": "2025-08-05T03:53:51.108202Z",
     "iopub.status.idle": "2025-08-05T03:53:57.452407Z",
     "shell.execute_reply": "2025-08-05T03:53:57.451806Z"
    },
    "papermill": {
     "duration": 6.348009,
     "end_time": "2025-08-05T03:53:57.453734",
     "exception": false,
     "start_time": "2025-08-05T03:53:51.105725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7de5f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T03:53:57.458729Z",
     "iopub.status.busy": "2025-08-05T03:53:57.458396Z",
     "iopub.status.idle": "2025-08-05T03:54:24.597603Z",
     "shell.execute_reply": "2025-08-05T03:54:24.596592Z"
    },
    "papermill": {
     "duration": 27.143996,
     "end_time": "2025-08-05T03:54:24.599626",
     "exception": false,
     "start_time": "2025-08-05T03:53:57.455630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process with rank=0, world_size=2 device='cuda:0'\n",
      "Starting process with rank=1, world_size=2 device='cuda:1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W805 03:54:05.917920006 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W805 03:54:05.924203798 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W805 03:54:13.919544309 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W805 03:54:21.921284104 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank1]:[W805 03:54:21.273145008 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W805 03:54:21.273141696 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{rank=} Finished init!!!\n",
      "{rank=} Finished init!!!\n",
      "\n",
      "Task 1 - all gather\n",
      "Process 0 Information...\n",
      "Process 1 Information...\n",
      "{rank=} Finished step 1!!!{rank=} Finished step 1!!!\n",
      "\n",
      "\n",
      "Task 2 - all reduce\n",
      "All reduce for all processes: in rank 0, tensor = tensor([2., 2., 2., 2.], device='cuda:0')All reduce for all processes: in rank 1, tensor = tensor([2., 2., 2., 2.], device='cuda:1')\n",
      "\n",
      "\n",
      "Task 3 - all reduce for sub-group\n",
      "Sub group all reduce: in rank 1, tensor = tensor([1., 1., 1., 1.], device='cuda:1')\n",
      "\n",
      "Rank 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\n",
      "In rank 1, group_1_sum.to('cpu')=tensor([1, 1, 1, 1]), group_2_sum.to('cpu')=tensor([1.5000, 1.5000, 1.5000, 1.5000])In rank 0, group_1_sum.to('cpu')=tensor([1, 1, 1, 1]), group_2_sum.to('cpu')=tensor([1.5000, 1.5000, 1.5000, 1.5000])\n",
      "\n",
      "\n",
      "Finishing process with rank=1, world_size=2\n",
      "Finishing process with rank=0, world_size=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Process for collective communication\n",
    "\n",
    "# 广播（Broadcast）：广播是一种将数据从一个源进程发送到所有其他进程的通信操作。在 torch.distributed 中，通过 broadcast(tensor, src=0) 可以实现该操作，将 rank 为 0 的进程中的数据广播到所有其他进程。广播操作能够确保所有进程拥有相同的数据，适合需要共享模型参数、初始化权重等场景。比如在分布式训练的初始化阶段，用于将主进程的模型参数广播到所有其他进程，保证训练从同样的初始参数开始。\n",
    "# 规约（Reduce 和 All-Reduce）：规约操作是一种将多个进程的数据进行计算（如求和、求最大值等）的操作。常用的规约操作有两种，reduce()：一个进程（通常是主进程）收集并合并来自所有进程的数据；all_reduce()：所有进程同时得到合并后的数据。比如 all_reduce(tensor, op=ReduceOp.SUM) 会在所有进程中求和，并将结果存放在每个进程的 tensor 中。规约操作能有效减少通信负担，适用于大规模梯度汇总或模型权重更新。譬如在分布式训练中，all_reduce 常用于梯度求和，以确保在多个进程中的梯度保持一致，实现同步更新。\n",
    "# 收集（Gather 和 All-Gather）：收集操作是将多个进程的数据收集到一个或多个进程的操作：gather()：将多个进程的数据收集到一个进程中。all_gather()：所有进程都收集到全部进程的数据。例如 all_gather(gathered_tensors, tensor) 会将所有进程中的 tensor 收集到每个进程的 gathered_tensors 列表中。收集操作方便对所有进程中的数据进行后续分析和处理。譬如做 evaluation 时，可以使用 all_gather 来汇总各个进程的中间结果。\n",
    "# 散发（Scatter）：scatter() 操作是将一个进程的数据分散到多个进程中。例如在 rank 为 0 的进程中有一个包含若干子张量的列表，scatter() 可以将列表中的每个子张量分配给其他进程。适用于数据分发，将大型数据集或模型权重在多个进程中分散，以便每个进程可以处理不同的数据块。\n",
    "\n",
    "def init_process(rank, world_size, backend=\"nccl\"):\n",
    "  device = f\"cuda:{rank}\"\n",
    "  print(f\"Starting process with {rank=}, {world_size=} {device=}\")\n",
    "\n",
    "  # Use the gloo backend for CPU-based distributed processing\n",
    "  dist.init_process_group(backend=\"nccl\", world_size=world_size, rank=rank)\n",
    "\n",
    "  assert rank == dist.get_rank()\n",
    "  assert world_size == dist.get_world_size()\n",
    "  dist.barrier()\n",
    "  print(\"{rank=} Finished init!!!\")\n",
    "\n",
    "  # Task 1 - all gather\n",
    "  # It gathers information from all nodes.\n",
    "  if rank == 0:\n",
    "    print(\"\\nTask 1 - all gather\")\n",
    "  process_info = (\n",
    "      f\"Process {rank} Information...\"\n",
    "  )\n",
    "  max_len = 100\n",
    "  process_info_tensor = torch.zeros(max_len, dtype=torch.int32).to(device)\n",
    "  process_info_bytes = process_info.encode('utf-8')\n",
    "  process_info_tensor[:len(process_info_bytes)] = torch.tensor([b for b in process_info_bytes], dtype=torch.int32)\n",
    "\n",
    "  gathered_tensors = [torch.zeros(max_len, dtype=torch.int32).to(device) for _ in range(world_size)]\n",
    "\n",
    "  dist.all_gather(gathered_tensors, process_info_tensor)\n",
    "\n",
    "  if rank == 0:\n",
    "    for t in gathered_tensors:\n",
    "      info_bytes = t.to('cpu').numpy().astype('uint8').tobytes()\n",
    "      info_str = info_bytes.decode('utf-8', 'ignore').strip('\\x00')\n",
    "      print(info_str)\n",
    "  dist.barrier()\n",
    "  print(\"{rank=} Finished step 1!!!\")\n",
    "\n",
    "  # Task 2 - all reduce (sum)\n",
    "  if rank == 0:\n",
    "    print(\"\\nTask 2 - all reduce\")\n",
    "  tensor = torch.ones((4,)).to(device)\n",
    "  dist.all_reduce(tensor)\n",
    "  print(f\"All reduce for all processes: in rank {rank}, tensor = {tensor}\")\n",
    "  dist.barrier()\n",
    "\n",
    "  # Task 3 - all reduce (sum) in a sub-group.\n",
    "  if rank == 0:\n",
    "    print(\"\\nTask 3 - all reduce for sub-group\")\n",
    "  sub_group_ranks = range(1, world_size, 2)\n",
    "  sub_group = dist.new_group(ranks=sub_group_ranks)\n",
    "  if rank in sub_group_ranks:\n",
    "    sub_group_tensor = torch.ones((4,)).to(device)\n",
    "    dist.all_reduce(sub_group_tensor, group=sub_group)\n",
    "    print(f\"Sub group all reduce: in rank {rank}, tensor = {sub_group_tensor}\")\n",
    "  dist.barrier()\n",
    "\n",
    "  # Task 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\n",
    "  if rank == 0:\n",
    "    print(\"\\nRank 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\")\n",
    "  group_1_sum = torch.tensor([1, 1, 1, 1]).to(device)\n",
    "  group_2_sum = torch.tensor([1.5] * 4).to(device)\n",
    "  group_1_ranks = list(range(world_size // 2))\n",
    "  group_2_ranks = list(range(world_size // 2, world_size))\n",
    "  group_1 = dist.new_group(ranks=group_1_ranks)\n",
    "  group_2 = dist.new_group(ranks=group_2_ranks)\n",
    "  if rank in group_1_ranks:\n",
    "    dist.all_reduce(group_1_sum, group=group_1)\n",
    "  else:\n",
    "    dist.all_reduce(group_2_sum, group=group_2)\n",
    "  # Communicate the sub-group sums to the entire group.\n",
    "  dist.all_reduce(group_1_sum, op=dist.ReduceOp.MAX)\n",
    "  dist.all_reduce(group_2_sum, op=dist.ReduceOp.MAX)\n",
    "  print(f\"In rank {rank}, {group_1_sum.to('cpu')=}, {group_2_sum.to('cpu')=}\")\n",
    "\n",
    "  # Finish\n",
    "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
    "  dist.destroy_process_group()\n",
    "\n",
    "# Run the distributed processing\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12459' # You can choose a different port if 12355 is in use\n",
    "\n",
    "world_size = 2\n",
    "\n",
    "processes = []\n",
    "for rank in range(world_size):\n",
    "  p = mp.Process(target=init_process, args=(rank, world_size))\n",
    "  p.start()\n",
    "  processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "  p.join()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.850533,
   "end_time": "2025-08-05T03:54:26.224274",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-05T03:53:43.373741",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
