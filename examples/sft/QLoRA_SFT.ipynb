{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNcxDWHKtnWM8/ZS/HAgqR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/examples/sft/QLoRA_SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA SFT training example.\n",
        "\n",
        "Tutorial: https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face"
      ],
      "metadata": {
        "id": "SzbB1fg0_z5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dependencies\n",
        "\n",
        "!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4"
      ],
      "metadata": {
        "id": "wyW2NPAtAQ5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "id": "4rOmOJH1AYGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a quantized base model"
      ],
      "metadata": {
        "id": "pWZ_xkOGAyla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "# repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "repo_id = 'Qwen/Qwen2.5-Coder-7B-Instruct'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "LV0TEyq7A0wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model memory footprints = {model.get_memory_footprint()/1e9:.1f}GB\")"
      ],
      "metadata": {
        "id": "xhVFdOYUBgCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "FHmaEnArBhWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up LoRA\n",
        "\n",
        "> A quantized model can be used directly for inference, but it cannot be trained any further. Those pesky Linear4bit layers take up much less space, which is the whole point of quantization; however, we cannot update them."
      ],
      "metadata": {
        "id": "BYc_lLfoB3-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call `prepare_model_for_kbit_training`, it performs the following key actions on your model:\n",
        "\n",
        "1. Casts LayerNorms to FP32.\n",
        "\n",
        "2. Enables Gradient Checkpointing.\n",
        "\n",
        "3. Makes Output Embeddings Trainable.\n",
        "\n",
        "4. Adds Forward Hooks"
      ],
      "metadata": {
        "id": "xTubYqlXDTps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "0qQ5oVFGDcFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
        "    r=8,\n",
        "    lora_alpha=16, # multiplier, usually 2*r\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Newer models, such as Phi-3 at time of writing, may require\n",
        "    # manually setting target modules\n",
        "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "model"
      ],
      "metadata": {
        "id": "YPT_-ASgCev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model preparation function (prepare_model_for_kbit_training()) turned\n",
        "# every non-quantized layer to full precision (FP32), thus resulting in a 30%\n",
        "# larger model:\n",
        "print(f\"The prepared model memory footprints = {model.get_memory_footprint()/1e9:.1f}GB\")\n",
        "\n",
        "train_p, tot_p = model.get_nb_trainable_parameters()\n",
        "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
        "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
        "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')"
      ],
      "metadata": {
        "id": "P7Oi8Qf9Dtna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset has three columns:\n",
        "\n",
        "1. original English sentence (sentence)\n",
        "2. basic translation to Yoda-speak (translation)\n",
        "3. enhanced translation including typical Yesss and Hrrmm interjections (translation_extra)"
      ],
      "metadata": {
        "id": "a2qCJgIxEBih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "j1UaL8zlED8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3HzruMMUElrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the DS to *Conversational Format*\n",
        "\n",
        "```\n",
        "{\"messages\":[\n",
        "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
        "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
        "]}\n",
        "```"
      ],
      "metadata": {
        "id": "RYYeNJy5EuoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "dataset = dataset.remove_columns([\"translation\"])\n",
        "dataset"
      ],
      "metadata": {
        "id": "TdQiwzR_E70k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "4Xa04SX-E9sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conversion Libs\n",
        "\n",
        "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
        "# Converts dataset from prompt/completion format (not supported anymore)\n",
        "# to the conversational format\n",
        "def format_dataset(examples):\n",
        "    if isinstance(examples[\"prompt\"], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[\"prompt\"])):\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "            ]\n",
        "            output_texts.append(converted_sample)\n",
        "        return {'messages': output_texts}\n",
        "    else:\n",
        "        converted_sample = [\n",
        "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "        ]\n",
        "        return {'messages': converted_sample}"
      ],
      "metadata": {
        "id": "6BaVF3OaFCQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])\n",
        "dataset[0]['messages']"
      ],
      "metadata": {
        "id": "rqee3IaxFG_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "V5ax-gKYFPOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "tokenizer.chat_template"
      ],
      "metadata": {
        "id": "lr4wWI2yFRUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of the formatted example\n",
        "print(tokenizer.apply_chat_template(dataset[0]['messages'], tokenize=False))"
      ],
      "metadata": {
        "id": "7Cct0UvpFlF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special handling for PI3\n",
        "\n",
        "> **IMPORTANT UPDATE**: due to changes in the default collator used by the SFTTrainer class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\n",
        ">\n",
        "> In order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels."
      ],
      "metadata": {
        "id": "l_n6Jw5tHfv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"Phi-3\" in repo_id:\n",
        "  tokenizer.pad_token = tokenizer.unk_token\n",
        "  tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "F1Y2BKUyHoyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SFT"
      ],
      "metadata": {
        "id": "Ek00wki6Ijl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = SFTConfig(\n",
        "    ## GROUP 1: Memory usage\n",
        "    # These arguments will squeeze the most out of your GPU's RAM\n",
        "    # Checkpointing\n",
        "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
        "    # Set this to avoid exceptions in newer versions of PyTorch\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    # Gradient Accumulation / Batch size\n",
        "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
        "    gradient_accumulation_steps=1,\n",
        "    # The initial (micro) batch size to start off with\n",
        "    per_device_train_batch_size=16,\n",
        "    # If batch size would cause OOM, halves its size until it works\n",
        "    auto_find_batch_size=True,\n",
        "\n",
        "    ## GROUP 2: Dataset-related\n",
        "    max_seq_length=64,\n",
        "    # Dataset\n",
        "    # packing a dataset means no padding is needed\n",
        "    packing=True,\n",
        "\n",
        "    ## GROUP 3: These are typical training parameters\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=3e-4,\n",
        "    # Optimizer\n",
        "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
        "    optim='paged_adamw_8bit',\n",
        "\n",
        "    ## GROUP 4: Logging parameters\n",
        "    logging_steps=10,\n",
        "    logging_dir='./logs',\n",
        "    output_dir='./phi3-mini-yoda-adapter',\n",
        "    report_to='none'\n",
        ")"
      ],
      "metadata": {
        "id": "-48HFRO4Itw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "kXkP2Zq7IwaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = trainer.get_train_dataloader()\n",
        "batch = next(iter(dl))\n",
        "\n",
        "# peek the DS\n",
        "batch['input_ids'][0], batch['labels'][0]"
      ],
      "metadata": {
        "id": "RPKfVJhkIzyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "1GPNvDQqI58s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}