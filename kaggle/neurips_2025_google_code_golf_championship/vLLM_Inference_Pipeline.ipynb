{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/kaggle/neurips_2025_google_code_golf_championship/vLLM_Inference_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F8ffCX6hluG"
      },
      "source": [
        "This colab is for the championship: https://www.kaggle.com/competitions/google-code-golf-2025/overview\n",
        "\n",
        "This colab run bulk inference using vLLM, and store the generated code into GCS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MCAVMrLc3cJ"
      },
      "source": [
        "# TLDR\n",
        "\n",
        "## Model Quality Comparison\n",
        "\n",
        "The 400 data points eval results are used to measure the quality.\n",
        "\n",
        "> Note: most of the model names are for Ollama; you can map to the equivalent vLLM (HF) model names yourself.\n",
        "\n",
        "| Model | thinking enabled | Valid Code Rate | Correct Code Rate |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **W3S** | True | 0.78 | 0.06 |\n",
        "| **W3M** | True | 0.47 | 0.04 |\n",
        "| **qwen2.5-coder:0.5b** | True | 0.04 | 0.00 |\n",
        "| **qwen2.5-coder:1.5b** | True | 0.15 | 0.00 |\n",
        "| **qwen2.5-coder:7b** | True | 0.34 | 0.01 |\n",
        "| **qwen2.5-coder:14b** | True | 0.39 | 0.03 |\n",
        "| **qwen3-coder:30b** | True | 0.61 | 0.11 |\n",
        "| **deepseek-coder:1.3b** | True | 0.00 | 0.00 |\n",
        "| **deepseek-coder:6.7b** | True | 0.19 | 0.01 |\n",
        "| **deepseek-coder:33b** | True | 0.20 | 0.05 |\n",
        "| **cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit** (FP8 KV quant) | True | 0.51 | 0.07 |\n",
        "| **cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit** (FP8 KV quant) | False | 0.53 | 0.06 |\n",
        "\n",
        "\n",
        "## Model Feasibility Comparison\n",
        "\n",
        "| Model | `max_model_len` | `kv_cache_dtype` | GPU | Result |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Qwen/Qwen3-4B** | `default` | `default` | L4 | ✅ Works |\n",
        "| | `default` | `default` | A100 | ✅ Works |\n",
        "| **Qwen/Qwen3-8B** | `default` | `default` | L4 | ❌ Doesn't Work |\n",
        "| | `default` | `default` | A100 | ✅ Works |\n",
        "| | `4096` | `default` | L4 | ✅ Works |\n",
        "| **Qwen/Qwen3-14B** | `default` | `default` | L4 | ❌ Doesn't Work |\n",
        "| | `default` | `default` | A100 | ✅ Works |\n",
        "| | `2048` | `default` | L4 | ❌ Doesn't Work |\n",
        "| **Qwen/Qwen3-32B** | `3072` | `default` | A100 | ❌ Doesn't Work |\n",
        "| **Qwen3-Coder-30B-FP8** | `3072` | `default` | A100 | ✅ Works |\n",
        "| | `4096` | `default` | A100 | ❌ Doesn't Work |\n",
        "| **Qwen3-Coder-30B-AWQ** | `4K` | `default` | L4 | ✅ Works |\n",
        "| | `5K` | `default` | L4 | ❌ Doesn't Work |\n",
        "| | `12K` | `fp8` | L4 | ✅ Works |\n",
        "| | `16K` | `fp8` | L4 | ❌ Doesn't Work |\n",
        "\n",
        "## Performance\n",
        "\n",
        "| `MODEL_ID` | `GPU` | `Engine` | `MAX_MODEL_LEN` | `kv_cache_dtype` | Thinking | Speed | Throughput |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| `cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit` | L4 | vLLM | `8192` | `fp8` | True | `7s/it` | `200 toks/s` |\n",
        "| `Qwen/Qwen3-32B` | L4 | `Ollama` | `default` | `N/A` | True | `43s/it` | `?` |\n",
        "| `cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit` | A100 | vLLM | `8192` | `fp8` | True | `15s/it` | `122 toks/s` |\n",
        "| `cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit` | A100 | vLLM | `8192` | `fp8` | False | `7.3s/it` | `122 toks/s` |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMTwWJ1l7w0p"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzw0TDwKCEvo"
      },
      "source": [
        "# Public Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6yuz_LGCGIQ"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "import dataclasses\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DataPoint:\n",
        "  # A list of (input, output) tuples\n",
        "  # train: list[tuple[np.ndarray, np.ndarray]]\n",
        "  train: list[tuple[Sequence[Sequence[int]], Sequence[Sequence[int]]]]\n",
        "  train_raw: list[dict]\n",
        "\n",
        "  # (input, output) tuple\n",
        "  # test: list[tuple[np.ndarray, np.ndarray]]\n",
        "  test: list[tuple[Sequence[Sequence[int]], Sequence[Sequence[int]]]]\n",
        "  test_raw: list[dict]\n",
        "\n",
        "  json_dict: dict[str, Any]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97-P-sWCLhp"
      },
      "source": [
        "# Download data and load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-JIbMl0ISxm"
      },
      "outputs": [],
      "source": [
        "# @title Setup Kaggle credential\n",
        "\n",
        "# Option 1 - Load kaggle secret from the Colab's Secrets\n",
        "#\n",
        "# This requires to save the download kaggle secrect json file's content to the colab's Secrets with name \"kaggle\"\n",
        "import os\n",
        "from google.colab import userdata\n",
        "kaggle_secret_json = userdata.get('kaggle')\n",
        "os.environ['env_var_kaggle_secret_json'] = kaggle_secret_json\n",
        "\n",
        "!mkdir -p ~/.kaggle/ && > ~/.kaggle/kaggle.json && echo $env_var_kaggle_secret_json >> ~/.kaggle/kaggle.json && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Option 2 - Upload kaggle secret from local file\n",
        "#\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# # Then move kaggle.json into the folder where the API expects to find it.\n",
        "# !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByO3f-wLDLGK"
      },
      "outputs": [],
      "source": [
        "# @title Download data\n",
        "\n",
        "%%capture\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "base_path = \"/content/google-code-golf-2025\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  !pip install --user kaggle\n",
        "  !kaggle competitions download -c google-code-golf-2025\n",
        "  !unzip /content/google-code-golf-2025.zip -d /content/google-code-golf-2025/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HHWAuqfCNa4"
      },
      "outputs": [],
      "source": [
        "# @title Parse the data\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Attempting to list files in: {base_path}\")\n",
        "\n",
        "# This returns a list of filename strings. It doesn't include the path.\n",
        "files = os.listdir(base_path)\n",
        "json_file_paths = [\n",
        "    os.path.join(base_path, f) for f in files if f.endswith(\".json\")\n",
        "]\n",
        "print(f\"Found {len(json_file_paths)} json files\")\n",
        "\n",
        "def extract_data_point(json_filepath: str) -> DataPoint:\n",
        "  \"\"\"Extract DataPoint from a json file.\n",
        "\n",
        "  Args:\n",
        "    json_filepath: The path to the json file.\n",
        "\n",
        "  Returns:\n",
        "    A DataPoint object.\n",
        "  \"\"\"\n",
        "  with open(json_filepath, \"rt\") as my_file:\n",
        "    content = my_file.read()\n",
        "    json_dict = json.loads(content)\n",
        "\n",
        "    train_val = json_dict[\"train\"]\n",
        "    all_train = [\n",
        "        (it[\"input\"], it[\"output\"]) for it in train_val\n",
        "    ]\n",
        "    train_raw = train_val\n",
        "\n",
        "    test_val = json_dict[\"test\"]\n",
        "    all_test = [\n",
        "        (it[\"input\"], it[\"output\"]) for it in test_val\n",
        "    ]\n",
        "    test_raw = test_val\n",
        "\n",
        "    return DataPoint(\n",
        "        train=all_train, train_raw=train_raw, test=all_test, test_raw=test_raw, json_dict=json_dict\n",
        "    )\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(\n",
        "    max_workers=min(512, len(json_file_paths)), thread_name_prefix=\"Worker\"\n",
        ") as executor:\n",
        "  # The map() function is the key.\n",
        "  # It applies 'worker_task' to each item in 'items_to_process'.\n",
        "  # It automatically collects the results and returns them as an iterator.\n",
        "  data_points = list(\n",
        "      tqdm.tqdm(\n",
        "          executor.map(extract_data_point, json_file_paths),\n",
        "          total=len(json_file_paths),\n",
        "      )\n",
        "  )\n",
        "  print(f\"{len(data_points)=}\")\n",
        "  print(f\"An example of the data points {data_points[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DcKiVfvh07o"
      },
      "outputs": [],
      "source": [
        "# @title Install vLLM and dependencies\n",
        "\n",
        "%%capture\n",
        "\n",
        "# The 0.10 version of vLLM doesn't load model successfully. So we use 0.9.2 version\n",
        "\n",
        "# Similar bugs\n",
        "#   https://github.com/vllm-project/vllm/issues/17618\n",
        "!pip install vllm==0.9.2 lm-format-enforcer pandas\n",
        "\n",
        "# This is needed to be compatible with vLLM 0.9.2.\n",
        "#\n",
        "# For issue: https://github.com/vllm-project/vllm/issues/17618\n",
        "!pip install \"transformers<4.54.0\"\n",
        "\n",
        "!pip show vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS8L68qeLgcQ"
      },
      "source": [
        "# Generation libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zHO__T-B7Bb"
      },
      "outputs": [],
      "source": [
        "# @title Visualization libs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Optional, for slightly nicer colorbars and default styles\n",
        "\n",
        "def visualize_np_array(np_array):\n",
        "    \"\"\"\n",
        "    Visualizes a 2D NumPy array (rectangular matrix of integers between 0 and 9)\n",
        "    as a heatmap.\n",
        "\n",
        "    Args:\n",
        "        np_array (np.ndarray): The NumPy array to visualize.\n",
        "                               Expected shape: (rows, cols)\n",
        "                               Expected values: integers between 0 and 9.\n",
        "    \"\"\"\n",
        "    if not isinstance(np_array, np.ndarray):\n",
        "        np_array = np.array(np_array)\n",
        "\n",
        "    if np_array.ndim != 2:\n",
        "        print(f\"Error: Input array must be 2-dimensional, but has {np_array.ndim} dimensions.\")\n",
        "        return\n",
        "\n",
        "    rows, cols = np_array.shape\n",
        "\n",
        "    if not (1 <= rows <= 30 and 1 <= cols <= 30):\n",
        "        print(f\"Error: Array dimensions ({rows}x{cols}) are outside the allowed range (1x1 to 30x30).\")\n",
        "        return\n",
        "\n",
        "    # Check if all values are integers between 0 and 9\n",
        "    if not (np.all(np_array >= 0) and np.all(np_array <= 9) and np.all(np_array == np_array.astype(int))):\n",
        "        print(\"Warning: Array contains values outside the 0-9 integer range. Visualization might be misleading.\")\n",
        "        # Attempt to cast to int to prevent issues with imshow expecting numeric data\n",
        "        np_array = np_array.astype(int)\n",
        "\n",
        "    # Set up the plot\n",
        "    image_zoom_factor = 1.0 if rows <= 10 and cols <= 10 else 0.5\n",
        "    plt.figure(figsize=(cols * image_zoom_factor, rows * image_zoom_factor)) # Adjust figure size dynamically for better aspect ratio\n",
        "\n",
        "    # Use seaborn's heatmap for a more aesthetically pleasing visualization\n",
        "    # 'cmap' defines the color map. 'viridis' is a good default for sequential data.\n",
        "    # 'RdYlGn' (Red-Yellow-Green) or 'Greens' are also good options.\n",
        "    # 'annot=True' will display the value in each cell (useful for small grids)\n",
        "    # 'fmt=\"d\"' ensures the annotation is an integer\n",
        "    # 'cbar=True' shows the color bar\n",
        "    # 'linewidths' and 'linecolor' add borders between cells\n",
        "    sns.heatmap(np_array, annot=True, fmt=\"d\", cmap=\"viridis\", cbar=True,\n",
        "                linewidths=0.5, linecolor='black', vmin=0, vmax=9)\n",
        "\n",
        "    plt.title(f'Visualization of {rows}x{cols} NumPy Array')\n",
        "    plt.xlabel('Column Index')\n",
        "    plt.ylabel('Row Index')\n",
        "    plt.xticks(np.arange(cols) + 0.5, labels=np.arange(cols)) # Center ticks\n",
        "    plt.yticks(np.arange(rows) + 0.5, labels=np.arange(rows)) # Center ticks\n",
        "    plt.gca().invert_yaxis() # Invert y-axis to have (0,0) at top-left like typical arrays\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "array = np.array([[0, 7, 7], [7, 7, 7], [0, 7, 7]])\n",
        "visualize_np_array(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRqlDZxmLhqO"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Libs\n",
        "SYSTEM_TURN = \"\"\"You are a principle software engineer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "USER_TURN_PREFIX = \"\"\"You should implement a Python function to do a transformation, which is implicitly described by pairs of <input, output> image grids. The transformation may include rotation, cropping, magnification, etc. Your code should achieve the desired result across all exemplars, and uses the fewest possible number of characters.\n",
        "\n",
        "A \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The size is between 1x1 and the 30x30.\n",
        "\n",
        "The function name is \"fn\". Its signature is `fn(input: typing.Sequence[typing.Sequence[int]]) -> typing.Sequence[typing.Sequence[int]]:`.\n",
        "\n",
        "Do NOT use any external library!\n",
        "\n",
        "Examplars:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_prompt(data_point: DataPoint) -> list[dict[str, str]]:\n",
        "  user_turn = USER_TURN_PREFIX\n",
        "\n",
        "  # Remove the whitespace in list of integers string to save tokens.\n",
        "  grid_pairs_str = str(data_point.train_raw)\n",
        "  len_before = len(grid_pairs_str)\n",
        "  grid_pairs_str = grid_pairs_str.replace(\", \", \",\")\n",
        "  user_turn += grid_pairs_str\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": SYSTEM_TURN},\n",
        "      {\"role\": \"user\", \"content\": user_turn},\n",
        "  ]\n",
        "  return messages\n",
        "\n",
        "\n",
        "# Test\n",
        "conversation = build_prompt(data_points[0])\n",
        "print(f\"Prompt = {conversation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIbw8ttGZBhe"
      },
      "outputs": [],
      "source": [
        "# @title Generate Code Libs\n",
        "\n",
        "def extract_code_from_resp(resp_str: str) -> str:\n",
        "  prefix = \"```python\"\n",
        "  suffix = \"```\"\n",
        "  while True:\n",
        "    prefix_idx = resp_str.find(prefix)\n",
        "    if prefix_idx != -1:\n",
        "      resp_str = resp_str[prefix_idx + len(prefix) :]\n",
        "    else:\n",
        "      break\n",
        "  suffix_idx = resp_str.find(suffix)\n",
        "  if suffix_idx != -1:\n",
        "    resp_str = resp_str[: suffix_idx]\n",
        "  return resp_str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzKQS_bPLnxW"
      },
      "outputs": [],
      "source": [
        "# @title JIT Python execution libs\n",
        "\n",
        "import typing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def exec_and_ret(code_str: str, input: np.ndarray) -> typing.Any:\n",
        "  \"\"\"Execute a string of code and return the result.\n",
        "\n",
        "  The result must be assigned to a variable named 'ret'.\n",
        "\n",
        "  Args:\n",
        "    code_str: A string of code.\n",
        "    input: the input to the code.\n",
        "\n",
        "  Returns:\n",
        "    The result of the code.\n",
        "  \"\"\"\n",
        "  loc = {}\n",
        "  exec(code_str, {'input': input}, loc)\n",
        "  return loc['ret']\n",
        "\n",
        "\n",
        "# # Test\n",
        "# code_str = \"\"\"ret = 1 + 2\"\"\"\n",
        "# assert 3 == exec_and_ret(code_str, input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r1RDn0lMe4M"
      },
      "outputs": [],
      "source": [
        "# @title Verification libs\n",
        "!pip install func-timeout\n",
        "\n",
        "from typing import Any\n",
        "from func_timeout import func_timeout, FunctionTimedOut\n",
        "\n",
        "\n",
        "def verify_code_dp(\n",
        "    code_str: str, data_point: DataPoint, visualize: bool = False, timeout_seconds:int = 30\n",
        ") -> tuple[bool, Any, str|None]:\n",
        "  \"\"\"Verify the given code on its expected result.\"\"\"\n",
        "  assert data_point.test\n",
        "  # inp = data_point.test[0][0].tolist()\n",
        "  # expected_output = data_point.test[0][1].tolist()\n",
        "  inp = data_point.test[0][0]\n",
        "  expected_output = data_point.test[0][1]\n",
        "  return verify_code(code_str, inp, expected_output, visualize)\n",
        "\n",
        "def execute_with_timeout(\n",
        "    code_str: str, inp: Sequence[Sequence[int]], timeout_seconds:int = 10\n",
        ") -> tuple[Any|None, str|None]:\n",
        "  \"\"\"Execute the code.\n",
        "\n",
        "  Args:\n",
        "    code_str: the str of the code.\n",
        "    inp: the input to the code.\n",
        "    timeout_seconds: the timeout in seconds.\n",
        "\n",
        "  Returns:\n",
        "    [0]: the code's output.\n",
        "    [1]: if the code is not executed successfully, return the err msg.\n",
        "  \"\"\"\n",
        "\n",
        "  # This is needed to emit the return of the execution.\n",
        "  code_str += \"\"\"\n",
        "ret = fn(input)\"\"\"\n",
        "\n",
        "  # Execute the code with a timeout to prevent culprit code never returns.\n",
        "  try:\n",
        "    # The return value of successful_function is captured in 'result'\n",
        "    output = func_timeout(timeout_seconds, exec_and_ret, args=(code_str, inp))\n",
        "    return output, None\n",
        "  except FunctionTimedOut:\n",
        "    err_msg = f\"🛑 The function was killed after {timeout_seconds} seconds.\\n\"\n",
        "    return None, err_msg\n",
        "  except Exception as e:\n",
        "    stack_trace = traceback.extract_stack()\n",
        "    err_msg = f\"Error: {e}. Stack trace: {stack_trace}\"\n",
        "    return None, err_msg\n",
        "\n",
        "\n",
        "def verify_code(\n",
        "    code_str: str, inp: Sequence[Sequence[int]], expected_output: Sequence[Sequence[int]], visualize: bool = False\n",
        ") -> tuple[bool, Any, str|None]:\n",
        "  \"\"\"Verify the code.\n",
        "\n",
        "  Args:\n",
        "    code_str: the str of the code.\n",
        "    inp: the input to the code.\n",
        "    expected_output: the expected output.\n",
        "    visualize: whether to visualize the execution output.\n",
        "\n",
        "  Returns:\n",
        "    [0]: whether the execution output matches the expected output.\n",
        "    [1]: the code's output.\n",
        "    [2]: if the code is not executed successfully, return the err msg.\n",
        "  \"\"\"\n",
        "  output, err_msg = execute_with_timeout(code_str, inp)\n",
        "  if output is None:\n",
        "    return False, output, err_msg\n",
        "\n",
        "  assert err_msg is None\n",
        "  try:\n",
        "    if visualize:\n",
        "      visualize_np_array(output)\n",
        "      visualize_np_array(expected_output)\n",
        "    output_np = np.array(output)\n",
        "    expected_output_np =np.array(expected_output)\n",
        "    if output_np.shape != expected_output_np.shape:\n",
        "      return False, output, err_msg\n",
        "    is_match = np.allclose(np.array(output), np.array(expected_output))\n",
        "    return is_match, output, err_msg\n",
        "  except Exception as e:\n",
        "    stack_trace = traceback.extract_stack()\n",
        "    err_msg = f\"Error: {e}. Stack trace: {stack_trace}\"\n",
        "    return False, None, err_msg\n",
        "\n",
        "# # Test\n",
        "# is_match, output, expected_output = verify_code_dp(\n",
        "#     \"\"\"def fn(inp):\n",
        "#       return 1\"\"\",\n",
        "#     data_points[0],\n",
        "# )\n",
        "# print(is_match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSQ4sd6iqVpx"
      },
      "outputs": [],
      "source": [
        "# @title Storage Libs\n",
        "\n",
        "from typing import Mapping, Sequence\n",
        "\n",
        "import os\n",
        "\n",
        "def persist_data_locally(file_path: str, data: Sequence[Mapping[str, Any]]) -> None:\n",
        "  \"\"\"Persist the data to a file and store the data in jsonl format.\n",
        "\n",
        "  Args:\n",
        "    file_path: the path to the file\n",
        "    data: the data to store. Each element is stored as a json.\n",
        "  \"\"\"\n",
        "  # Create dir if necessary\n",
        "  base_path = os.path.dirname(file_path)\n",
        "  if not os.path.exists(base_path):\n",
        "    os.makedirs(base_path)\n",
        "\n",
        "  # Will overwrite the file content.\n",
        "  with open(file_path, 'w', encoding='utf-8') as f:\n",
        "    for entry in data:\n",
        "      # Convert dictionary to a JSON string\n",
        "      json_str = json.dumps(entry)\n",
        "      # Write the JSON string to the file, followed by a newline\n",
        "      f.write(json_str + '\\n')\n",
        "    f.flush()\n",
        "\n",
        "# Unit Test\n",
        "data = [{\n",
        "    \"1\": \"2\",\n",
        "    \"3\": \"4\",\n",
        "},\n",
        "        {\n",
        "    \"1\": \"2\",\n",
        "    \"3\": \"45\",\n",
        "}]\n",
        "test_file_path = \"/content/tmp/test.jsonl\"\n",
        "persist_data_locally(test_file_path, data)\n",
        "\n",
        "parsed_data = []\n",
        "with open(test_file_path, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "\n",
        "    parsed_data.append(json.loads(line.strip()))\n",
        "\n",
        "assert data == parsed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYpg9CDsgZMo"
      },
      "source": [
        "# Run evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17Jc3H06qmI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Inference Libs\n",
        "\n",
        "def batch_inference(llm, data_points: list[DataPoint], enable_thinking: bool = True) -> Sequence[tuple[Mapping[str, str], str]]:\n",
        "  \"\"\"Run batch inference.\n",
        "\n",
        "  Args:\n",
        "    llm: the vllm instance.\n",
        "    data_points: the data to run inference.\n",
        "    enable_thinking: whether to enable thinking mode.\n",
        "\n",
        "  Returns:\n",
        "    [0]: the input prompt.\n",
        "    [1]: the inference result.\n",
        "  \"\"\"\n",
        "  # Process input\n",
        "  formatted_prompts_tokenized = []\n",
        "\n",
        "  tokenizer = llm.get_tokenizer()\n",
        "\n",
        "  input_cutoff_len = MAX_MODEL_LEN * 3 // 4\n",
        "  examples_over_max_ctx_length = []\n",
        "  prompts = []\n",
        "\n",
        "  for dp in data_points:\n",
        "      messages = build_prompt(dp)\n",
        "      prompts.append(messages)\n",
        "\n",
        "      input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
        "\n",
        "      if len(input_ids) > input_cutoff_len:\n",
        "        # print(f\"⚠️ Warning: Prompt is too long ({len(input_ids)} tokens). Truncating to {input_cutoff_len} tokens.\")\n",
        "        examples_over_max_ctx_length.append(input_ids)\n",
        "\n",
        "        # Keep the most recent tokens by slicing from the end\n",
        "        input_ids = input_ids[-input_cutoff_len:]\n",
        "\n",
        "      formatted_prompts_tokenized.append(input_ids)\n",
        "  print(f\"There are {len(examples_over_max_ctx_length)} examples over the max context length limit {input_cutoff_len}!\")\n",
        "\n",
        "  # Inference\n",
        "  sampling_params = vllm.SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2048)\n",
        "  tokens_prompts = [vllm.inputs.TokensPrompt(prompt_token_ids=it) for it in formatted_prompts_tokenized]\n",
        "\n",
        "  inference_start = time.time()\n",
        "  outputs = llm.generate(prompts=tokens_prompts, sampling_params=sampling_params)\n",
        "  inference_end = time.time()\n",
        "\n",
        "  inference_time = inference_end - inference_start\n",
        "  print(f\"Elapsed time: {inference_time:.1f} seconds\")\n",
        "\n",
        "  # Process output\n",
        "  generated_codes = []\n",
        "  for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    generated_code = extract_code_from_resp(generated_text)\n",
        "    generated_codes.append(generated_code)\n",
        "  return [(p, gc) for p, gc in zip(prompts, generated_codes)]\n",
        "\n",
        "def persist_inference_results(results: list[str], fname: str):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwaT17D8iD13"
      },
      "outputs": [],
      "source": [
        "# @title Eval Libs\n",
        "\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def batch_eval(llm, data_points: list[DataPoint], generated_codes: list[str]) -> list[bool, str]:\n",
        "  \"\"\"Run batch eval.\n",
        "\n",
        "  Args:\n",
        "    llm: the llm.\n",
        "    data_points: the input data points.\n",
        "    generated_codes: the generated codes.\n",
        "\n",
        "  Returns:\n",
        "    [0]: whether the generated code pass the test.\n",
        "    [1]: if the code fails to run, the corresponding err msg.\n",
        "  \"\"\"\n",
        "\n",
        "  # Eval the outputs\n",
        "  eval_results = []\n",
        "  for code, dp in zip(generated_codes, data_points):\n",
        "    is_match, _, err_msg = verify_code_dp(code, dp)\n",
        "    eval_results.append((is_match, err_msg))\n",
        "\n",
        "  return eval_results\n",
        "\n",
        "def generate_fixed_length_int_str(number: int, target_length: int = 6):\n",
        "  \"\"\"\n",
        "  Generates a fixed-length string (6) of digits with leading zeros.\n",
        "\n",
        "  Args:\n",
        "    number: An integer.\n",
        "\n",
        "  Returns:\n",
        "    A string of length 6, with leading zeros and the given number at the end.\n",
        "    Returns an error message if the number is too large to fit in 6 digits.\n",
        "  \"\"\"\n",
        "  if not isinstance(number, int) or number < 0:\n",
        "    raise ValueError(\"Input must be a non-negative integer.\")\n",
        "\n",
        "  str_number = str(number)\n",
        "\n",
        "  if len(str_number) > target_length:\n",
        "    raise ValueError(f\"Number {number} is too large to fit in {target_length} digits.\")\n",
        "\n",
        "  # Pad with leading zeros\n",
        "  padded_string = str_number.zfill(target_length)\n",
        "  return padded_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7glN7PRyEbXU"
      },
      "outputs": [],
      "source": [
        "# @title Download model to local files\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "MODEL_ID = \"cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit\"\n",
        "\n",
        "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "local_model_path = f\"./{MODEL_ID.split('/')[-1]}\" # e.g., ./Llama-2-7b-chat-hf\n",
        "\n",
        "if os.path.exists(local_model_path):\n",
        "  print(f\"The model is already on disk. Skip downloading ... {local_model_path}\")\n",
        "else:\n",
        "  print(f\"Downloading model to {local_model_path}...\")\n",
        "  snapshot_download(\n",
        "      repo_id=MODEL_ID,\n",
        "      local_dir=local_model_path,\n",
        "      local_dir_use_symlinks=False # Set to False to download files directly\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RukVn5OFMvE"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the vLLM from local files\n",
        "\n",
        "import vllm\n",
        "import os\n",
        "\n",
        "MAX_MODEL_LEN = 1024 * 8\n",
        "\n",
        "import torch\n",
        "\n",
        "llm = vllm.LLM(\n",
        "    model=local_model_path,\n",
        "    trust_remote_code=True,\n",
        "    max_model_len=MAX_MODEL_LEN,\n",
        "    gpu_memory_utilization=0.95,\n",
        "    kv_cache_dtype=\"fp8\",\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup GCS\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.colab import userdata\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Get Service Account credential\n",
        "data_stroage_secrect = userdata.get('gdrive_data_storage_service_account')\n",
        "\n",
        "data_storage_secret_file = \"/content/data_storage_secret.json\"\n",
        "\n",
        "with open(data_storage_secret_file, 'w') as f:\n",
        "  f.write(data_stroage_secrect)\n",
        "\n",
        "# Set credential\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = data_storage_secret_file\n",
        "\n",
        "# Connect to the bucket\n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'lizhi_general_storage'\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "print(f\"Accessing bucket: {bucket.name}\")\n",
        "\n",
        "def persist_data_to_gcs(bucket, data: Sequence[Mapping[str, Any]], filename: str):\n",
        "  content = \"\"\n",
        "  for entry in data:\n",
        "    # Convert dictionary to a JSON string\n",
        "    json_str = json.dumps(entry)\n",
        "    # Write the JSON string to the file, followed by a newline\n",
        "    content += (json_str + \"\\n\")\n",
        "\n",
        "  # Get the current date and time\n",
        "\n",
        "  blob_dir = 'neurips_2025_google_code_golf_championship/inference_outputs'\n",
        "  blob_path = os.path.join(blob_dir, filename)\n",
        "  blob = bucket.blob(blob_path)\n",
        "\n",
        "  blob.upload_from_string(content)\n",
        "  print(f\"'{blob_path}' uploaded to bucket '{bucket_name}'.\")"
      ],
      "metadata": {
        "id": "A-O9TozCzLkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWEsRgmSOya9"
      },
      "outputs": [],
      "source": [
        "# @title Run eval\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "print(f\"Evaluating **{MODEL_ID}** ...\\n\")\n",
        "\n",
        "CHUNK_SIZE = 50\n",
        "\n",
        "data_points_split = data_points[:300]\n",
        "for epoch in range(10):\n",
        "  print(f\"====================== Starting epoch {epoch+1} ======================\")\n",
        "\n",
        "  # Inference\n",
        "  inference_dps = data_points_split[:]\n",
        "  dps_chunks = [inference_dps[i:i + CHUNK_SIZE] for i in range(0, len(inference_dps), CHUNK_SIZE)]\n",
        "  inference_outputs = []\n",
        "  inference_start = time.time()\n",
        "\n",
        "  now = datetime.datetime.now()\n",
        "  inference_outputs_filename_prefix = \"inference_outputs_\" + now.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "  for batch, dps_chunk in enumerate(dps_chunks):\n",
        "    print(f\"Inferencing batch {batch} ...\")\n",
        "    inference_outputs_chunk = batch_inference(llm, dps_chunk, enable_thinking=False)\n",
        "    inference_outputs.extend(inference_outputs_chunk)\n",
        "\n",
        "    # Persist inference results\n",
        "    predictions = []\n",
        "    for dp, (prompt, gc) in zip(dps_chunk, inference_outputs_chunk):\n",
        "      predictions.append({\n",
        "          \"train\": dp.train_raw,\n",
        "          \"test\": dp.test_raw,\n",
        "          \"prompt\": prompt,\n",
        "          \"pred\": gc,\n",
        "      })\n",
        "\n",
        "    filename = inference_outputs_filename_prefix + f\"_{generate_fixed_length_int_str(batch)}.jsonl\"\n",
        "\n",
        "    # # Persist to the local Colab fs\n",
        "    # inference_results_path = \"/content/data/prediction_data.jsonl\"\n",
        "    # persist_data_locally(os.path.join(\"/content/data/\", filename), predictions)\n",
        "    # print(f\"Inference data is stored to {inference_results_path}\")\n",
        "\n",
        "    # Persist to GCS\n",
        "    persist_data_to_gcs(bucket, predictions, filename)\n",
        "\n",
        "  inference_end = time.time()\n",
        "  print(f\"Inference took {(inference_end - inference_start) / 60:.1f}mins\")\n",
        "\n",
        "\n",
        "  # Eval\n",
        "  generated_codes = [it for (_, it) in inference_outputs]\n",
        "  eval_results = batch_eval(llm, data_points_split, generated_codes)\n",
        "\n",
        "  is_matches = [result[0] for result in eval_results]\n",
        "  errors = [result[1] for result in eval_results]\n",
        "\n",
        "  num_valid_code = errors.count(None)\n",
        "  num_invalid_code = errors.count(not None)\n",
        "  num_correct_code = sum(is_matches)\n",
        "  num_incorrect_code = num_valid_code - num_correct_code\n",
        "\n",
        "  print(\n",
        "      f\"Valid code rate = {num_valid_code / len(eval_results):.2f}\"\n",
        "  )\n",
        "  print(\n",
        "      f\"Correct code rate =\"\n",
        "      f\" {num_correct_code / len(eval_results):.2f}\"\n",
        "  )\n",
        "  print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys7qztEDVwNw"
      },
      "source": [
        "# [Optional] Delete vLLM instance and release VRAM\n",
        "\n",
        "> [!WARNING]\n",
        "> This approach is not reliable in Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooxDuONGFeW1"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "try:\n",
        "  del llm\n",
        "except NameError as e:\n",
        "  pass\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# Deletes all unused tensors\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNT0/D+WIv4X/igP9Z5D/qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}