{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNBIo7tlAmtNWf5eCqBT4lm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/kaggle/neurips_2025_google_code_golf_championship/Eval_with_vLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab is for the championship: https://www.kaggle.com/competitions/google-code-golf-2025/overview\n",
        "\n",
        "Note\n",
        "- The Ollama Eval colab: https://github.com/lizhieffe/llm_knowledge/blob/main/kaggle/neurips_2025_google_code_golf_championship/Eval_with_Ollama.ipynb"
      ],
      "metadata": {
        "id": "4F8ffCX6hluG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TLDR\n",
        "\n",
        "## Model Quality Comparison\n",
        "\n",
        "The 400 data points eval results are used to measure the quality.\n",
        "\n",
        "> Note: most of the model names are for Ollama; you can map to the equivalent vLLM (HF) model names yourself.\n",
        "\n",
        "| Model | Valid Code Rate | Correct Code Rate |\n",
        "| :--- | :--- | :--- |\n",
        "| **W3S** | 0.78 | 0.06 |\n",
        "| **W3M** | 0.47 | 0.04 |\n",
        "| **qwen2.5-coder:0.5b** | 0.04 | 0.00 |\n",
        "| **qwen2.5-coder:1.5b** | 0.15 | 0.00 |\n",
        "| **qwen2.5-coder:7b** | 0.34 | 0.01 |\n",
        "| **qwen2.5-coder:14b** | 0.39 | 0.03 |\n",
        "| **qwen3-coder:30b** | 0.61 | 0.11 |\n",
        "| **deepseek-coder:1.3b** | 0.00 | 0.00 |\n",
        "| **deepseek-coder:6.7b** | 0.19 | 0.01 |\n",
        "| **deepseek-coder:33b** | 0.20 | 0.05 |\n",
        "| **cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit** (FP8 KV quant) | 0.51 | 0.07 |\n",
        "\n",
        "## Model Feasibility Comparison\n",
        "\n",
        "| Model | `max_model_len` | `kv_cache_dtype` | GPU | Result |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Qwen/Qwen3-4B** | `default` | `default` | L4 | ‚úÖ Works |\n",
        "| | `default` | `default` | A100 | ‚úÖ Works |\n",
        "| **Qwen/Qwen3-8B** | `default` | `default` | L4 | ‚ùå Doesn't Work |\n",
        "| | `default` | `default` | A100 | ‚úÖ Works |\n",
        "| | `4096` | `default` | L4 | ‚úÖ Works |\n",
        "| **Qwen/Qwen3-14B** | `default` | `default` | L4 | ‚ùå Doesn't Work |\n",
        "| | `default` | `default` | A100 | ‚úÖ Works |\n",
        "| | `2048` | `default` | L4 | ‚ùå Doesn't Work |\n",
        "| **Qwen/Qwen3-32B** | `3072` | `default` | A100 | ‚ùå Doesn't Work |\n",
        "| **Qwen3-Coder-30B-FP8** | `3072` | `default` | A100 | ‚úÖ Works |\n",
        "| | `4096` | `default` | A100 | ‚ùå Doesn't Work |\n",
        "| **Qwen3-Coder-30B-AWQ** | `4K` | `default` | L4 | ‚úÖ Works |\n",
        "| | `5K` | `default` | L4 | ‚ùå Doesn't Work |\n",
        "| | `12K` | `fp8` | L4 | ‚úÖ Works |\n",
        "| | `16K` | `fp8` | L4 | ‚ùå Doesn't Work |\n",
        "\n",
        "## Performance\n",
        "\n",
        "| `MODEL_NAME` | `GPU` | `Engine` | `MAX_MODEL_LEN` | `kv_cache_dtype` | Speed | Throughput |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- |  :--- |\n",
        "| `cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit` | L4 | vLLM | `8192` | `fp8` | `7s/it` | `200 toks/s` |\n",
        "| `Qwen/Qwen3-32B` | L4 | `Ollama` | `default` | `N/A` | `43s/it` | `?` |\n",
        "| `cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit` | A100 | vLLM | `8192` | `fp8` | `15s/it` | `122 toks/s` |\n",
        "\n"
      ],
      "metadata": {
        "id": "_MCAVMrLc3cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public Interface"
      ],
      "metadata": {
        "id": "Vzw0TDwKCEvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DataPoint:\n",
        "  # A list of (input, output) tuples\n",
        "  train: list[tuple[np.ndarray, np.ndarray]]\n",
        "  train_raw: list[dict]\n",
        "\n",
        "  # (input, output) tuple\n",
        "  test: list[tuple[np.ndarray, np.ndarray]]\n",
        "  test_raw: list[dict]"
      ],
      "metadata": {
        "id": "i6yuz_LGCGIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data and load"
      ],
      "metadata": {
        "id": "a97-P-sWCLhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Kaggle credential\n",
        "\n",
        "# Option 1 - Load kaggle secret from the Colab's Secrets\n",
        "#\n",
        "# This requires to save the download kaggle secrect json file's content to the colab's Secrets with name \"kaggle\"\n",
        "import os\n",
        "from google.colab import userdata\n",
        "kaggle_secret_json = userdata.get('kaggle')\n",
        "os.environ['env_var_kaggle_secret_json'] = kaggle_secret_json\n",
        "\n",
        "!mkdir -p ~/.kaggle/ && > ~/.kaggle/kaggle.json && echo $env_var_kaggle_secret_json >> ~/.kaggle/kaggle.json && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Option 2 - Upload kaggle secret from local file\n",
        "#\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# # Then move kaggle.json into the folder where the API expects to find it.\n",
        "# !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2-JIbMl0ISxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download data\n",
        "\n",
        "%%capture\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "base_path = \"/content/google-code-golf-2025\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  !pip install --user kaggle\n",
        "  !kaggle competitions download -c google-code-golf-2025\n",
        "  !unzip /content/google-code-golf-2025.zip -d /content/google-code-golf-2025/"
      ],
      "metadata": {
        "id": "ByO3f-wLDLGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parse the data\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Attempting to list files in: {base_path}\")\n",
        "\n",
        "# This returns a list of filename strings. It doesn't include the path.\n",
        "files = os.listdir(base_path)\n",
        "json_file_paths = [\n",
        "    os.path.join(base_path, f) for f in files if f.endswith(\".json\")\n",
        "]\n",
        "print(f\"Found {len(json_file_paths)} json files\")\n",
        "\n",
        "def extract_data_point(json_filepath: str) -> DataPoint:\n",
        "  \"\"\"Extract DataPoint from a json file.\n",
        "\n",
        "  Args:\n",
        "    json_filepath: The path to the json file.\n",
        "\n",
        "  Returns:\n",
        "    A DataPoint object.√è\n",
        "  \"\"\"\n",
        "  with open(json_filepath, \"rt\") as my_file:\n",
        "    content = my_file.read()\n",
        "    # print(content)\n",
        "    json_dict = json.loads(content)\n",
        "    # print(type(json_dict))\n",
        "\n",
        "    train_val = json_dict[\"train\"]\n",
        "    all_train = [\n",
        "        (np.array(it[\"input\"]), np.array(it[\"output\"])) for it in train_val\n",
        "    ]\n",
        "    train_raw = train_val\n",
        "\n",
        "    test_val = json_dict[\"train\"]\n",
        "    all_test = [\n",
        "        (np.array(it[\"input\"]), np.array(it[\"output\"])) for it in test_val\n",
        "    ]\n",
        "    test_raw = test_val\n",
        "\n",
        "    return DataPoint(\n",
        "        train=all_train, train_raw=train_raw, test=all_test, test_raw=test_raw\n",
        "    )\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(\n",
        "    max_workers=min(512, len(json_file_paths)), thread_name_prefix=\"Worker\"\n",
        ") as executor:\n",
        "  # The map() function is the key.\n",
        "  # It applies 'worker_task' to each item in 'items_to_process'.\n",
        "  # It automatically collects the results and returns them as an iterator.\n",
        "  data_points = list(\n",
        "      tqdm.tqdm(\n",
        "          executor.map(extract_data_point, json_file_paths),\n",
        "          total=len(json_file_paths),\n",
        "      )\n",
        "  )\n",
        "  print(f\"{len(data_points)=}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6HHWAuqfCNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install vLLM and dependencies\n",
        "\n",
        "%%capture\n",
        "\n",
        "# The 0.10 version of vLLM doesn't load model successfully. So we use 0.9.2 version\n",
        "\n",
        "# Similar bugs\n",
        "#   https://github.com/vllm-project/vllm/issues/17618\n",
        "!pip install vllm==0.9.2 lm-format-enforcer pandas\n",
        "\n",
        "# This is needed to be compatible with vLLM 0.9.2.\n",
        "#\n",
        "# For issue: https://github.com/vllm-project/vllm/issues/17618\n",
        "!pip install \"transformers<4.54.0\"\n",
        "\n",
        "!pip show vllm"
      ],
      "metadata": {
        "id": "-DcKiVfvh07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation libs"
      ],
      "metadata": {
        "id": "zS8L68qeLgcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualization libs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Optional, for slightly nicer colorbars and default styles\n",
        "\n",
        "def visualize_np_array(np_array):\n",
        "    \"\"\"\n",
        "    Visualizes a 2D NumPy array (rectangular matrix of integers between 0 and 9)\n",
        "    as a heatmap.\n",
        "\n",
        "    Args:\n",
        "        np_array (np.ndarray): The NumPy array to visualize.\n",
        "                               Expected shape: (rows, cols)\n",
        "                               Expected values: integers between 0 and 9.\n",
        "    \"\"\"\n",
        "    if not isinstance(np_array, np.ndarray):\n",
        "        np_array = np.array(np_array)\n",
        "\n",
        "    if np_array.ndim != 2:\n",
        "        print(f\"Error: Input array must be 2-dimensional, but has {np_array.ndim} dimensions.\")\n",
        "        return\n",
        "\n",
        "    rows, cols = np_array.shape\n",
        "\n",
        "    if not (1 <= rows <= 30 and 1 <= cols <= 30):\n",
        "        print(f\"Error: Array dimensions ({rows}x{cols}) are outside the allowed range (1x1 to 30x30).\")\n",
        "        return\n",
        "\n",
        "    # Check if all values are integers between 0 and 9\n",
        "    if not (np.all(np_array >= 0) and np.all(np_array <= 9) and np.all(np_array == np_array.astype(int))):\n",
        "        print(\"Warning: Array contains values outside the 0-9 integer range. Visualization might be misleading.\")\n",
        "        # Attempt to cast to int to prevent issues with imshow expecting numeric data\n",
        "        np_array = np_array.astype(int)\n",
        "\n",
        "    # Set up the plot\n",
        "    image_zoom_factor = 1.0 if rows <= 10 and cols <= 10 else 0.5\n",
        "    plt.figure(figsize=(cols * image_zoom_factor, rows * image_zoom_factor)) # Adjust figure size dynamically for better aspect ratio\n",
        "\n",
        "    # Use seaborn's heatmap for a more aesthetically pleasing visualization\n",
        "    # 'cmap' defines the color map. 'viridis' is a good default for sequential data.\n",
        "    # 'RdYlGn' (Red-Yellow-Green) or 'Greens' are also good options.\n",
        "    # 'annot=True' will display the value in each cell (useful for small grids)\n",
        "    # 'fmt=\"d\"' ensures the annotation is an integer\n",
        "    # 'cbar=True' shows the color bar\n",
        "    # 'linewidths' and 'linecolor' add borders between cells\n",
        "    sns.heatmap(np_array, annot=True, fmt=\"d\", cmap=\"viridis\", cbar=True,\n",
        "                linewidths=0.5, linecolor='black', vmin=0, vmax=9)\n",
        "\n",
        "    plt.title(f'Visualization of {rows}x{cols} NumPy Array')\n",
        "    plt.xlabel('Column Index')\n",
        "    plt.ylabel('Row Index')\n",
        "    plt.xticks(np.arange(cols) + 0.5, labels=np.arange(cols)) # Center ticks\n",
        "    plt.yticks(np.arange(rows) + 0.5, labels=np.arange(rows)) # Center ticks\n",
        "    plt.gca().invert_yaxis() # Invert y-axis to have (0,0) at top-left like typical arrays\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "array = np.array([[0, 7, 7], [7, 7, 7], [0, 7, 7]])\n",
        "visualize_np_array(array)"
      ],
      "metadata": {
        "id": "1zHO__T-B7Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prompt Libs\n",
        "SYSTEM_TURN = \"\"\"You are a principle software engineer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "USER_TURN_PREFIX = \"\"\"You should implement a Python function to do a transformation, where the transformation is implicitly described by multiple pair of <input, output> image grids that are given. For example, the example pairs for one task might demonstrate the concept of rotation, whereas another might involve cropping and/or magnification. Your code for a given task should not only achieve the desired result across all exemplars, but also do so using the fewest possible number of characters.\n",
        "\n",
        "A \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The smallest possible grid size is 1x1 and the largest is 30x30.\n",
        "\n",
        "The function should have name \"fn\", one input arg named \"input\".\n",
        "\n",
        "Do NOT use any external library!\n",
        "\n",
        "Examplars:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_prompt(data_point: DataPoint) -> list[dict[str, str]]:\n",
        "  user_turn = USER_TURN_PREFIX\n",
        "  user_turn += str(data_point.train_raw)\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": SYSTEM_TURN},\n",
        "      {\"role\": \"user\", \"content\": user_turn},\n",
        "  ]\n",
        "  return messages\n",
        "\n",
        "\n",
        "# Test\n",
        "conversation = build_prompt(data_points[0])\n",
        "print(f\"Prompt = {conversation}\")"
      ],
      "metadata": {
        "id": "wRqlDZxmLhqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Code Libs\n",
        "\n",
        "def extract_code_from_resp(resp_str: str) -> str:\n",
        "  prefix = \"```python\"\n",
        "  suffix = \"```\"\n",
        "  while True:\n",
        "    prefix_idx = resp_str.find(prefix)\n",
        "    if prefix_idx != -1:\n",
        "      resp_str = resp_str[prefix_idx + len(prefix) :]\n",
        "    else:\n",
        "      break\n",
        "  suffix_idx = resp_str.find(suffix)\n",
        "  if suffix_idx != -1:\n",
        "    resp_str = resp_str[: suffix_idx]\n",
        "  return resp_str.strip()"
      ],
      "metadata": {
        "id": "HIbw8ttGZBhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JIT Python execution libs\n",
        "\n",
        "import typing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def exec_and_ret(code_str: str, input: np.ndarray) -> typing.Any:\n",
        "  \"\"\"Execute a string of code and return the result.\n",
        "\n",
        "  The result must be assigned to a variable named 'ret'.\n",
        "\n",
        "  Args:\n",
        "    code_str: A string of code.\n",
        "    input: the input to the code.\n",
        "\n",
        "  Returns:\n",
        "    The result of the code.\n",
        "  \"\"\"\n",
        "  loc = {}\n",
        "  exec(code_str, {'input': input}, loc)\n",
        "  return loc['ret']\n",
        "\n",
        "\n",
        "# # Test\n",
        "# code_str = \"\"\"ret = 1 + 2\"\"\"\n",
        "# assert 3 == exec_and_ret(code_str, input)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FzKQS_bPLnxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Verification libs\n",
        "!pip install func-timeout\n",
        "\n",
        "from typing import Any\n",
        "from func_timeout import func_timeout, FunctionTimedOut\n",
        "\n",
        "\n",
        "def verify_code(\n",
        "    code_str: str, data_point: DataPoint, visualize: bool = False, timeout_seconds:int = 30\n",
        ") -> tuple[bool, list[Any], list[Any]]:\n",
        "  \"\"\"Verify the given code on its expected result.\"\"\"\n",
        "\n",
        "  # This is needed to emit the return of the execution.\n",
        "  code_str += \"\"\"\n",
        "ret = fn(input)\"\"\"\n",
        "\n",
        "  assert data_point.test\n",
        "  inp = data_point.test[0][0].tolist()\n",
        "  expected_output = data_point.test[0][1].tolist()\n",
        "\n",
        "  # Execute the code with a timeout to prevent culprit code never returns.\n",
        "  try:\n",
        "    # The return value of successful_function is captured in 'result'\n",
        "    output = func_timeout(timeout_seconds, exec_and_ret, args=(code_str, inp))\n",
        "\n",
        "    if visualize:\n",
        "      visualize_np_array(output)\n",
        "      visualize_np_array(expected_output)\n",
        "\n",
        "    is_match = np.allclose(np.array(output), np.array(expected_output))\n",
        "\n",
        "    return is_match, output, expected_output\n",
        "  except FunctionTimedOut:\n",
        "    print(f\"üõë The function was killed after {timeout_seconds} seconds.\\n\")\n",
        "    return False, None, expected_output\n",
        "\n",
        "\n",
        "# # Test\n",
        "# is_match, output, expected_output = verify_code(\n",
        "#     \"\"\"def fn(inp):\n",
        "#       return 1\"\"\",\n",
        "#     data_points[0],\n",
        "# )\n",
        "# print(is_match)"
      ],
      "metadata": {
        "id": "_r1RDn0lMe4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evals"
      ],
      "metadata": {
        "id": "oYpg9CDsgZMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Init the vLLM\n",
        "import vllm\n",
        "import os\n",
        "\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-0.6B\") # OK\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-4B\") # OK\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-8B\", max_model_len=4096) # OK on A100. OK on L4 after max_model_len=4096.\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-14B\", max_model_len=2048) # OK on A100. Not working on L4 after max_model_len=2048, 3072 or 4096\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-32B\", max_model_len=3072) # Not working on A100 after max_model_len=3072, 4096\n",
        "# llm = vllm.LLM(model=\"Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\", max_model_len=4096) # OK on A100 for 3072; hasn't try 4096. For L4, 4096 doesn't work.\n",
        "\n",
        "MODEL_NAME = \"cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit\"\n",
        "MAX_MODEL_LEN = 1024 * 8\n",
        "\n",
        "llm = vllm.LLM(\n",
        "    model=MODEL_NAME,\n",
        "    max_model_len=MAX_MODEL_LEN,\n",
        "    # gpu_memory_utilization=0.95,\n",
        "    kv_cache_dtype=\"fp8\",\n",
        "    ) # L4: non-fp8 OK 4096, Not OK 5120, 6144, 8192; fp8: OK 8192, 1024*12, Not OK 1024*16"
      ],
      "metadata": {
        "id": "ZzLvAm2qibLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Eval Libs\n",
        "\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def run_eval_loop():\n",
        "  # Process input\n",
        "  formatted_prompts_tokenized = []\n",
        "\n",
        "  tokenizer = llm.get_tokenizer()\n",
        "\n",
        "  input_cutoff_len = MAX_MODEL_LEN * 3 // 4\n",
        "  examples_over_max_ctx_length = []\n",
        "\n",
        "  for dp in data_points:\n",
        "      messages = build_prompt(dp)\n",
        "      input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, enable_thinking=True)\n",
        "\n",
        "\n",
        "      if len(input_ids) > input_cutoff_len:\n",
        "        # print(f\"‚ö†Ô∏è Warning: Prompt is too long ({len(input_ids)} tokens). Truncating to {input_cutoff_len} tokens.\")\n",
        "        examples_over_max_ctx_length.append(input_ids)\n",
        "\n",
        "        # Keep the most recent tokens by slicing from the end\n",
        "        input_ids = input_ids[-input_cutoff_len:]\n",
        "\n",
        "      formatted_prompts_tokenized.append(input_ids)\n",
        "  print(f\"There are {len(examples_over_max_ctx_length)} examples over the max context length limit {input_cutoff_len}!\")\n",
        "\n",
        "  # Inference\n",
        "  sampling_params = vllm.SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2048)\n",
        "\n",
        "\n",
        "  tokens_prompts = [vllm.inputs.TokensPrompt(prompt_token_ids=it) for it in formatted_prompts_tokenized]\n",
        "\n",
        "  inference_start = time.time()\n",
        "  outputs = llm.generate(prompts=tokens_prompts, sampling_params=sampling_params)\n",
        "  inference_end = time.time()\n",
        "  inference_time = inference_end - inference_start\n",
        "  print(f\"Elapsed time: {inference_time:.1f} seconds\")\n",
        "\n",
        "  # Process output\n",
        "  generated_codes = []\n",
        "  for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    generated_code = extract_code_from_resp(generated_text)\n",
        "    generated_codes.append(generated_code)\n",
        "\n",
        "  # Eval the outputs\n",
        "  eval_results = []\n",
        "  for code, dp in zip(generated_codes, data_points):\n",
        "    try:\n",
        "      is_match, _, _ = verify_code(code, dp)\n",
        "      eval_results.append((is_match, None))\n",
        "    except Exception as e:\n",
        "      stack_trace = traceback.extract_stack()\n",
        "      err_msg = f\"Error: {e}. Stack trace: {stack_trace}\"\n",
        "      err = ValueError(err_msg)\n",
        "      eval_results.append((False, err))\n",
        "\n",
        "  return eval_results"
      ],
      "metadata": {
        "id": "HwaT17D8iD13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Run eval\n",
        "\n",
        "import time\n",
        "\n",
        "inference_start = time.time()\n",
        "eval_results = run_eval_loop()\n",
        "\n",
        "is_matches = [result[0] for result in eval_results]\n",
        "errors = [result[1] for result in eval_results]\n",
        "\n",
        "num_valid_code = errors.count(None)\n",
        "num_invalid_code = errors.count(not None)\n",
        "num_correct_code = sum(is_matches)\n",
        "num_incorrect_code = num_valid_code - num_correct_code\n",
        "\n",
        "print(f\"Evaluating **{MODEL_NAME}** ...\")\n",
        "print(\n",
        "    f\"Valid code rate = {num_valid_code / len(eval_results):.2f}\"\n",
        ")\n",
        "print(\n",
        "    f\"Correct code rate =\"\n",
        "    f\" {num_correct_code / len(eval_results):.2f}\"\n",
        ")\n",
        "print(\"=\" * 80)\n",
        "print(\"=\" * 80)\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "hWEsRgmSOya9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}