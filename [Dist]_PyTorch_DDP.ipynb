{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdL2+FPhtMBDDTnIboUAbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/%5BDist%5D_PyTorch_DDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reference: https://zhuanlan.zhihu.com/p/178402798\n",
        "- Good read about the mechanism:\n",
        "  - https://zhuanlan.zhihu.com/p/187610959"
      ],
      "metadata": {
        "id": "1uxUUCXZ9_T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single GPU"
      ],
      "metadata": {
        "id": "U6XXpSa1FRZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Init\n",
        "input = torch.randn(20, 10).to(DEVICE) # (20, 10)\n",
        "labels = torch.randn(20, 10).to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "model = torch.nn.Linear(10, 10).to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for it in range(1):\n",
        "  # forward\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(input)\n",
        "\n",
        "  # backward\n",
        "  loss_fn(outputs, labels).backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # check model params\n",
        "  print(f\"In epoch {it}\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f\"{name=}, {param.data=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g90S9hU6BQ2D",
        "outputId": "d96f895c-7836-4568-f069-d732feb19c31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0\n",
            "name='weight', param.data=tensor([[ 0.0944, -0.1266, -0.1251,  0.1248, -0.0659, -0.0834,  0.1643, -0.1748,\n",
            "          0.1602,  0.2234],\n",
            "        [-0.1549, -0.2573,  0.0233, -0.1648,  0.1694, -0.1869, -0.2008, -0.0400,\n",
            "         -0.0904, -0.2047],\n",
            "        [ 0.1007,  0.1481,  0.0845,  0.1624,  0.0025,  0.2104, -0.2733,  0.2917,\n",
            "          0.1926, -0.1286],\n",
            "        [-0.0789, -0.1440,  0.2480,  0.0249,  0.0506,  0.2015, -0.1588,  0.2132,\n",
            "         -0.2207, -0.1388],\n",
            "        [-0.0530, -0.2940, -0.2188, -0.2619, -0.1186, -0.1148, -0.0541,  0.0963,\n",
            "         -0.0057, -0.1375],\n",
            "        [-0.2125,  0.2258, -0.2172, -0.1571, -0.1958,  0.2219,  0.2494,  0.0186,\n",
            "         -0.1865,  0.0155],\n",
            "        [ 0.0577,  0.0483,  0.0066, -0.1421, -0.0218,  0.0573, -0.0655,  0.0892,\n",
            "         -0.1105,  0.0901],\n",
            "        [-0.0250,  0.1317, -0.1487, -0.2025, -0.2007,  0.1900,  0.1915,  0.1950,\n",
            "          0.1023,  0.0138],\n",
            "        [-0.0253, -0.2263, -0.0107, -0.1052, -0.0309,  0.1138, -0.0093, -0.1307,\n",
            "          0.0476, -0.0651],\n",
            "        [-0.1008, -0.1295, -0.1835,  0.1902,  0.0472, -0.1306,  0.1623,  0.0707,\n",
            "          0.1605,  0.1529]])\n",
            "name='bias', param.data=tensor([ 0.2977,  0.0091, -0.0333, -0.1386,  0.2557,  0.1151, -0.0040,  0.0774,\n",
            "         0.2714,  0.1051])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDP\n",
        "\n",
        "- Here we use CPU instead of GPU to reduce the system requirement.\n"
      ],
      "metadata": {
        "id": "2McyiArZFVIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try 1 - Manually dist the training data.\n",
        "\n",
        "- We expect the trained model weights to be exactly the same as the single CPU scenario."
      ],
      "metadata": {
        "id": "dddZ7RyoRtNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "def run_single_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  split_data_size = 20 // 4\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  # Create the train set.\n",
        "  if rank == 0:\n",
        "    inputs = torch.randn(20, 10)\n",
        "    inputs_split_list = torch.split(inputs, split_data_size, dim=0)\n",
        "    inputs_split_list = list(inputs_split_list)\n",
        "    assert (20 // split_data_size) == len(inputs_split_list)\n",
        "\n",
        "    targets = torch.randn(20, 10)\n",
        "    targets_split_list = torch.split(targets, split_data_size, dim=0)\n",
        "    targets_split_list = list(targets_split_list)\n",
        "    assert (20 // split_data_size) == len(targets_split_list)\n",
        "  else:\n",
        "    inputs_split_list = None\n",
        "    targets_split_list = None\n",
        "\n",
        "  # Split the train set and send to the distributed workers.\n",
        "  inputs_split = torch.zeros((split_data_size, 10), dtype=torch.float32)\n",
        "  dist.scatter(inputs_split, inputs_split_list, src=0)\n",
        "  inputs_split.to(DEVICE)\n",
        "\n",
        "  targets_split = torch.zeros((split_data_size, 10), dtype=torch.float32)\n",
        "  dist.scatter(targets_split, targets_split_list, src=0)\n",
        "  targets_split.to(DEVICE)\n",
        "\n",
        "  # Init the model\n",
        "  model = torch.nn.Linear(10, 10).to(DEVICE)\n",
        "  ddp_model = DDP(model, device_ids=None)\n",
        "  loss_fn = torch.nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=1)\n",
        "\n",
        "  # forward\n",
        "  optimizer.zero_grad()\n",
        "  outputs = ddp_model(inputs_split)\n",
        "\n",
        "  # backward\n",
        "  loss_fn(outputs, targets_split).backward()\n",
        "\n",
        "  # check model params\n",
        "  # if rank == 0:\n",
        "  #   print(\"Before backward\")\n",
        "  #   for name, param in ddp_model.named_parameters():\n",
        "  #     if param.requires_grad:\n",
        "  #       print(f\"{name=}, {param.data=}\")\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # check model params\n",
        "  if rank == 0:\n",
        "    print(\"After backward\")\n",
        "    for name, param in ddp_model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        print(f\"{name=}, {param.data=}\")\n",
        "\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n",
        "\n",
        "world_size = 4\n",
        "\n",
        "processes = []\n",
        "for rank in range(world_size):\n",
        "  p = mp.Process(target=run_single_process, args=(rank, world_size))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAPDEnzOJMd5",
        "outputId": "00feb22f-eb4b-46ed-aa03-5067b1d6bb39"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4\n",
            "Starting process with rank=1, world_size=4\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n",
            "After backward\n",
            "name='module.weight', param.data=tensor([[ 0.0944, -0.1266, -0.1251,  0.1248, -0.0659, -0.0834,  0.1643, -0.1748,\n",
            "          0.1602,  0.2234],\n",
            "        [-0.1549, -0.2573,  0.0233, -0.1648,  0.1694, -0.1869, -0.2008, -0.0400,\n",
            "         -0.0904, -0.2047],\n",
            "        [ 0.1007,  0.1481,  0.0845,  0.1624,  0.0025,  0.2104, -0.2733,  0.2917,\n",
            "          0.1926, -0.1286],\n",
            "        [-0.0789, -0.1440,  0.2480,  0.0249,  0.0506,  0.2015, -0.1588,  0.2132,\n",
            "         -0.2207, -0.1388],\n",
            "        [-0.0530, -0.2940, -0.2188, -0.2619, -0.1186, -0.1148, -0.0541,  0.0963,\n",
            "         -0.0057, -0.1375],\n",
            "        [-0.2125,  0.2258, -0.2172, -0.1571, -0.1958,  0.2219,  0.2494,  0.0186,\n",
            "         -0.1865,  0.0155],\n",
            "        [ 0.0577,  0.0483,  0.0066, -0.1421, -0.0218,  0.0573, -0.0655,  0.0892,\n",
            "         -0.1105,  0.0901],\n",
            "        [-0.0250,  0.1317, -0.1487, -0.2025, -0.2007,  0.1900,  0.1915,  0.1950,\n",
            "          0.1023,  0.0138],\n",
            "        [-0.0253, -0.2263, -0.0107, -0.1052, -0.0309,  0.1138, -0.0093, -0.1307,\n",
            "          0.0476, -0.0651],\n",
            "        [-0.1008, -0.1295, -0.1835,  0.1902,  0.0472, -0.1306,  0.1623,  0.0707,\n",
            "          0.1605,  0.1529]])\n",
            "name='module.bias', param.data=tensor([ 0.2977,  0.0091, -0.0333, -0.1386,  0.2557,  0.1151, -0.0040,  0.0774,\n",
            "         0.2714,  0.1051])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try 2 - use distributed sampler"
      ],
      "metadata": {
        "id": "3awzclzMUJ8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "# This is the global bs. In DPP, it should guarantee the sum of bs on all devices equal to this #.\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "WORLD_SIZE = 4\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(ToyModel, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "      self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = x.view(-1, 16 * 5 * 5)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "# from the official doc: https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "      self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "dataset_transform = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])"
      ],
      "metadata": {
        "id": "ljM4jc72bxCl"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title single CPU version\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# Init the model\n",
        "model = ToyModel().to(DEVICE)\n",
        "model.train()\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Dataset\n",
        "download_path = \"./data\"\n",
        "my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n",
        "trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for it, (data, label) in enumerate(trainloader):\n",
        "\n",
        "    # forward\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "\n",
        "    # backward\n",
        "    loss = loss_fn(outputs, label)\n",
        "    loss.backward()\n",
        "\n",
        "    if it % 200 == 0:\n",
        "      print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9X0kAQlbsLS",
        "outputId": "076468ed-05d8-42b0-e575-fca8bb3b88b6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, it=0, loss=2.320\n",
            "epoch=0, it=200, loss=2.296\n",
            "epoch=0, it=400, loss=2.289\n",
            "epoch=0, it=600, loss=2.263\n",
            "epoch=0, it=800, loss=2.210\n",
            "epoch=0, it=1000, loss=2.294\n",
            "epoch=0, it=1200, loss=2.180\n",
            "epoch=0, it=1400, loss=2.228\n",
            "epoch=0, it=1600, loss=2.208\n",
            "epoch=0, it=1800, loss=1.816\n",
            "epoch=0, it=2000, loss=1.924\n",
            "epoch=0, it=2200, loss=1.912\n",
            "epoch=0, it=2400, loss=1.899\n",
            "epoch=0, it=2600, loss=1.794\n",
            "epoch=0, it=2800, loss=1.813\n",
            "epoch=0, it=3000, loss=1.949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dist version\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "def run_single_process(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=WORLD_SIZE, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  # Init the model\n",
        "  #\n",
        "  # DDP初始化（也就是model = DDP(model)这一步）\n",
        "  # 1. 把parameter，buffer从master节点传到其他节点，使所有进程上的状态一致。\n",
        "  #   注释：DDP通过这一步保证所有进程的初始状态一致。所以，请确保在这一步之后，你的代码不会再修改模型的任何东西了，包括添加、修改、删除parameter和buffer！\n",
        "  # 2.（可能）如果有每个节点有多卡，则在每张卡上创建模型（类似DP）\n",
        "  # 3. 把parameter进行分组，每一组称为一个bucket。临近的parameter在同一个bucket。\n",
        "  #   注释：这是为了加速，在梯度通讯时，先计算、得到梯度的bucket会马上进行通讯，不必等到所有梯度计算结束才进行通讯。后面会详细介绍。\n",
        "  # 4. 创建管理器reducer，给每个parameter注册梯度平均的hook。\n",
        "  #   注释：这一步的具体实现是在C++代码里面的，即reducer.h文件。\n",
        "  # 5.（可能）为可能的SyncBN层做准备\n",
        "  #\n",
        "  # 在每个step中，DDP模型都会做下面的事情：\n",
        "  # 1. 采样数据，从dataloader得到一个batch的数据，用于当前计算（for data, label in dataloader）。\n",
        "  #   注释：因为我们的dataloader使用了DistributedSampler，所以各个进程之间的数据是不会重复的。如果要确保DDP性能和单卡性能一致，这边需要保证在数据上，DDP模式下的一个epoch和单卡下的一个epoch是等效的。\n",
        "  # 2. 进行网络的前向计算（prediction = model(data)）\n",
        "  #   2.1 同步各进程状态\n",
        "  #     2.1.1（可能）对单进程多卡复制模式，要在进程内同步多卡之间的parameter和buffer\n",
        "  #     2.1.2 同步各进程之间的buffer。\n",
        "  #   2.2 接下来才是进行真正的前向计算\n",
        "  #   2.3（可能）当DDP参数find_unused_parameter为true时，其会在forward结束时，启动一个回溯，标记出所有没被用到的parameter，提前把这些设定为ready。\n",
        "  #     注释：find_unused_parameter的默认值是false，因为其会拖慢速度。\n",
        "  # 3. 计算梯度（loss.backward()）\n",
        "  #   3.1 reducer外面：各个进程各自开始反向地计算梯度。\n",
        "  #     3.1.1 注释：梯度是反向计算的，所以最后面的参数反而是最先得到梯度的。\n",
        "  #   3.2 reducer外面：当某个parameter的梯度计算好了的时候，其之前注册的grad hook就会被触发，在reducer里把这个parameter的状态标记为ready。\n",
        "  #   3.3 reducer里面：当某个bucket的所有parameter都是ready状态时，reducer会开始对这个bucket的所有parameter都开始一个异步的all-reduce梯度平均操作。\n",
        "  #     注释：\n",
        "  #       3.3.1 bucket的执行过程也是有顺序的，其顺序与parameter是相反的，即最先注册的parameter的bucket在最后面。\n",
        "  #       3.3.2 所以，我们在创建module的时候，请务必把先进行计算的parameter注册在前面，后计算的在后面。不然，reducer会卡在某一个bucket等待，使训练时间延长！\n",
        "  #         3.3.2.1 所谓的参数注册，其实就是创建网络层。也就是要求按照网络计算顺序，依次创建网络层。\n",
        "  #   3.4 reducer里面：当所有bucket的梯度平均都结束后，reducer才会把得到的平均grad结果正式写入到parameter.grad里面。\n",
        "  #   注释：这一步，感觉没有必要等全部结束之后才进行。可能得对照一下源码。\n",
        "  # 4. 优化器optimizer应用gradient，更新参数（optimizer.step()）。\n",
        "  #   注释：这一步，是和DDP没关系的。\n",
        "  model = ToyModel().to(DEVICE)\n",
        "  ddp_model = DDP(model, device_ids=None)\n",
        "  ddp_model.train()\n",
        "  loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "  # Init the optimizer.\n",
        "  #\n",
        "  # 我们可以看到，因为optimizer和DDP是没有关系的，所以optimizer初始状态的同一性是不被DDP保证的！\n",
        "  # 大多数官方optimizer，其实现能保证从同样状态的model初始化时，其初始状态是相同的。\n",
        "  # 所以这边我们只要保证在DDP模型创建后才初始化optimizer，就不用做额外的操作。\n",
        "  # 但是，如果自定义optimizer，则需要你自己来保证其统一性！\n",
        "  # 回顾一下文章最开始的代码，你会发现，optimizer确实是在DDP之后定义的。这个时候的模式已经是被初始化为相同的参数，所以能够保证优化器的初始状态是相同的。\n",
        "  optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Dataset\n",
        "  download_path = f\"./data_{rank}\"\n",
        "  my_trainset = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=dataset_transform)\n",
        "  # DDP：使用DistributedSampler，DDP帮我们把细节都封装起来了。\n",
        "  #      用，就完事儿！\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset)\n",
        "  # DDP：需要注意的是，这里的batch_size指的是每个进程下的batch_size。\n",
        "  #      也就是说，总batch_size是这里的batch_size再乘以并行数(world_size)。\n",
        "  assert BATCH_SIZE % WORLD_SIZE == 0\n",
        "  trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=BATCH_SIZE//WORLD_SIZE, sampler=train_sampler)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    # The distributed training loss is not going to be the same as the single device training.\n",
        "    # The reason is that the distributed sampler uses \"epoch\" as the sampling seed in each host.\n",
        "    #\n",
        "    # 不知道你有没有好奇，为什么给dataloader加一个DistributedSampler，就可以无缝对接DDP模式呢？\n",
        "    # 其实原理很简单，就是给不同进程分配数据集的不重叠、不交叉部分。\n",
        "    # 那么问题来了，每次epoch我们都会随机shuffle数据集，那么，不同进程之间要怎么保持shuffle后数据集的一致性呢？\n",
        "    # DistributedSampler的实现方式是，不同进程会使用一个相同的随机数种子，这样shuffle出来的东西就能确保一致。\n",
        "    #\n",
        "    # 具体实现上，DistributedSampler使用当前epoch作为随机数种子，从而使得不同epoch下有不同的shuffle结果。\n",
        "    # 所以，记得每次epoch开始前都要调用一下sampler的set_epoch方法，这样才能让数据集随机shuffle起来。\n",
        "    trainloader.sampler.set_epoch(epoch)\n",
        "\n",
        "    for it, (data, label) in enumerate(trainloader):\n",
        "\n",
        "      # forward\n",
        "      optimizer.zero_grad()\n",
        "      outputs = ddp_model(data)\n",
        "\n",
        "      # backward\n",
        "      loss = loss_fn(outputs, label)\n",
        "      loss.backward()\n",
        "\n",
        "      if rank == 0 and it % 200 == 0:\n",
        "        print(f\"{epoch=}, {it=}, loss={loss.item():.3f}\")\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355' # You can choose a different port if 12355 is in use\n",
        "\n",
        "processes = []\n",
        "for rank in range(WORLD_SIZE):\n",
        "  p = mp.Process(target=run_single_process, args=(rank, WORLD_SIZE))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez5NWWEDUQLj",
        "outputId": "8eaae427-b349-4400-b9e3-f1215770571e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4\n",
            "Starting process with rank=1, world_size=4\n",
            "Starting process with rank=2, world_size=4\n",
            "Starting process with rank=3, world_size=4\n",
            "epoch=0, it=0, loss=2.284\n",
            "epoch=0, it=200, loss=2.244\n",
            "epoch=0, it=400, loss=2.296\n",
            "epoch=0, it=600, loss=2.322\n",
            "epoch=0, it=800, loss=2.286\n",
            "epoch=0, it=1000, loss=2.185\n",
            "epoch=0, it=1200, loss=2.227\n",
            "epoch=0, it=1400, loss=1.905\n",
            "epoch=0, it=1600, loss=2.099\n",
            "epoch=0, it=1800, loss=1.635\n",
            "epoch=0, it=2000, loss=2.344\n",
            "epoch=0, it=2200, loss=1.547\n",
            "epoch=0, it=2400, loss=2.609\n",
            "epoch=0, it=2600, loss=2.036\n",
            "epoch=0, it=2800, loss=1.854\n",
            "epoch=0, it=3000, loss=1.633\n",
            "epoch=1, it=0, loss=1.158\n",
            "epoch=1, it=200, loss=1.877\n",
            "epoch=1, it=400, loss=2.356\n",
            "epoch=1, it=600, loss=1.611\n",
            "epoch=1, it=800, loss=1.564\n",
            "epoch=1, it=1000, loss=1.683\n",
            "epoch=1, it=1200, loss=0.823\n",
            "epoch=1, it=1400, loss=1.330\n",
            "epoch=1, it=1600, loss=1.527\n",
            "epoch=1, it=1800, loss=2.017\n",
            "epoch=1, it=2000, loss=1.623\n",
            "epoch=1, it=2200, loss=0.835\n",
            "epoch=1, it=2400, loss=1.886\n",
            "epoch=1, it=2600, loss=2.017\n",
            "epoch=1, it=2800, loss=1.238\n",
            "epoch=1, it=3000, loss=1.414\n"
          ]
        }
      ]
    }
  ]
}