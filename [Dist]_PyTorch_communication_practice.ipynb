{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdnLhq6HGzb92lQMqwemVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/llm_knowledge/blob/main/Torch_Dist_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch dist practice\n",
        "\n",
        "Reference\n",
        "- https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/distributed/torch-distributed/readme.md"
      ],
      "metadata": {
        "id": "2CR4DSKAwb6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zKG0cxEh5skn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process for collective communication\n",
        "\n",
        "# 广播（Broadcast）：广播是一种将数据从一个源进程发送到所有其他进程的通信操作。在 torch.distributed 中，通过 broadcast(tensor, src=0) 可以实现该操作，将 rank 为 0 的进程中的数据广播到所有其他进程。广播操作能够确保所有进程拥有相同的数据，适合需要共享模型参数、初始化权重等场景。比如在分布式训练的初始化阶段，用于将主进程的模型参数广播到所有其他进程，保证训练从同样的初始参数开始。\n",
        "# 规约（Reduce 和 All-Reduce）：规约操作是一种将多个进程的数据进行计算（如求和、求最大值等）的操作。常用的规约操作有两种，reduce()：一个进程（通常是主进程）收集并合并来自所有进程的数据；all_reduce()：所有进程同时得到合并后的数据。比如 all_reduce(tensor, op=ReduceOp.SUM) 会在所有进程中求和，并将结果存放在每个进程的 tensor 中。规约操作能有效减少通信负担，适用于大规模梯度汇总或模型权重更新。譬如在分布式训练中，all_reduce 常用于梯度求和，以确保在多个进程中的梯度保持一致，实现同步更新。\n",
        "# 收集（Gather 和 All-Gather）：收集操作是将多个进程的数据收集到一个或多个进程的操作：gather()：将多个进程的数据收集到一个进程中。all_gather()：所有进程都收集到全部进程的数据。例如 all_gather(gathered_tensors, tensor) 会将所有进程中的 tensor 收集到每个进程的 gathered_tensors 列表中。收集操作方便对所有进程中的数据进行后续分析和处理。譬如做 evaluation 时，可以使用 all_gather 来汇总各个进程的中间结果。\n",
        "# 散发（Scatter）：scatter() 操作是将一个进程的数据分散到多个进程中。例如在 rank 为 0 的进程中有一个包含若干子张量的列表，scatter() 可以将列表中的每个子张量分配给其他进程。适用于数据分发，将大型数据集或模型权重在多个进程中分散，以便每个进程可以处理不同的数据块。\n",
        "\n",
        "def init_process(rank, world_size):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  # Task 1 - all gather\n",
        "  # It gathers information from all nodes.\n",
        "  if rank == 0:\n",
        "    print(\"\\nTask 1 - all gather\")\n",
        "  process_info = (\n",
        "      f\"Process {rank} Information...\"\n",
        "  )\n",
        "  max_len = 100\n",
        "  process_info_tensor = torch.zeros(max_len, dtype=torch.int32)\n",
        "  process_info_bytes = process_info.encode('utf-8')\n",
        "  process_info_tensor[:len(process_info_bytes)] = torch.tensor([b for b in process_info_bytes], dtype=torch.int32)\n",
        "\n",
        "  gathered_tensors = [torch.zeros(max_len, dtype=torch.int32) for _ in range(world_size)]\n",
        "\n",
        "  dist.all_gather(gathered_tensors, process_info_tensor)\n",
        "\n",
        "  if rank == 0:\n",
        "    for t in gathered_tensors:\n",
        "      info_bytes = t.numpy().astype('uint8').tobytes()\n",
        "      info_str = info_bytes.decode('utf-8', 'ignore').strip('\\x00')\n",
        "      print(info_str)\n",
        "  dist.barrier()\n",
        "\n",
        "  # Task 2 - all reduce (sum)\n",
        "  if rank == 0:\n",
        "    print(\"\\nTask 2 - all reduce\")\n",
        "  tensor = torch.ones((4,))\n",
        "  dist.all_reduce(tensor)\n",
        "  print(f\"All reduce for all processes: in rank {rank}, tensor = {tensor}\")\n",
        "  dist.barrier()\n",
        "\n",
        "  # Task 3 - all reduce (sum) in a sub-group.\n",
        "  if rank == 0:\n",
        "    print(\"\\nTask 3 - all reduce for sub-group\")\n",
        "  sub_group_ranks = [1, 3]\n",
        "  sub_group = dist.new_group(ranks=sub_group_ranks)\n",
        "  if rank in sub_group_ranks:\n",
        "    sub_group_tensor = torch.ones((4,))\n",
        "    dist.all_reduce(sub_group_tensor, group=sub_group)\n",
        "    print(f\"Sub group all reduce: in rank {rank}, tensor = {sub_group_tensor}\")\n",
        "  dist.barrier()\n",
        "\n",
        "  # Task 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\n",
        "  if rank == 0:\n",
        "    print(\"\\nRank 4 - all reduce (sum) in a sub-group, then sync results to the entire group.\")\n",
        "  group_1_sum = torch.tensor([1, 1, 1, 1])\n",
        "  group_2_sum = torch.tensor([1.5] * 4)\n",
        "  group_1_ranks = list(range(world_size // 2))\n",
        "  group_2_ranks = list(range(world_size // 2, world_size))\n",
        "  group_1 = dist.new_group(ranks=group_1_ranks)\n",
        "  group_2 = dist.new_group(ranks=group_2_ranks)\n",
        "  if rank in group_1_ranks:\n",
        "    dist.all_reduce(group_1_sum, group=group_1)\n",
        "  else:\n",
        "    dist.all_reduce(group_2_sum, group=group_2)\n",
        "  # Communicate the sub-group sums to the entire group.\n",
        "  dist.all_reduce(group_1_sum, op=dist.ReduceOp.MAX)\n",
        "  dist.all_reduce(group_2_sum, op=dist.ReduceOp.MAX)\n",
        "  print(f\"In rank {rank}, {group_1_sum=}, {group_2_sum=}\")\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()\n",
        "\n",
        "\n",
        "\n",
        "# Colab doesn't support mp.spawn(), so we use mp.Process() to create the processes.\n",
        "# mp.spawn(\n",
        "#     init_process,\n",
        "#     args=(1, 1,),\n",
        "#     nprocs=1,\n",
        "#     join=True\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OmS55Lc3A4xY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process for P2P communication\n",
        "\n",
        "# 点对点通信是最基础的通信模式，指的是一个进程直接向另一个特定的进程发送或接收数据。这种模式非常灵活，适合需要精确控制通信过程的场景。\n",
        "\n",
        "# send-receive 模式：在 torch.distributed 中，这种模式可以通过 send() 和 recv() 接口实现。\n",
        "# 比如 send(tensor, dst=1) 表示进程将数据发送给 rank 为 1 的进程，而 recv(tensor, src=0) 表示接收来自 rank 为 0 的进程的数据。毫无疑问，这是阻塞式的。\n",
        "# 点对点通信的优点是简单直观，易于理解和控制；缺点是容易导致复杂的代码结构，尤其在需要多进程相互发送数据的情况下，可能会出现死锁或阻塞问题。因此，这种方式更多适用于两个进程之间的信息交换。适合需要精确控制单个进程之间数据交换的场景，通常在系统层通信优化中或模型分片时使用较多。例如在模型并行训练的梯度更新中，点对点通信可以用于梯度的汇总。\n",
        "\n",
        "def init_process_p2p(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  if rank == 0:\n",
        "    tensor = torch.tensor([100, 200], dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    dist.send(tensor, dst=1)\n",
        "    print(f\"Rank 0 sent data to Rank 1 ...\")\n",
        "  elif rank == 1:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    dist.recv(tensor, src=0)\n",
        "    print(f\"Rank 1 received data from Rank 0 ...\")\n",
        "\n",
        "    tensor += 100\n",
        "    print(f\"Modified data in {rank=} is: {tensor}\")\n",
        "    dist.send(tensor, dst=2)\n",
        "    print(f\"Rank 1 sent data to Rank 2 ...\")\n",
        "  elif rank == 2:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    dist.recv(tensor, src=1)\n",
        "    print(f\"Rank 2 received data from Rank 1 ...\")\n",
        "  else:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "\n",
        "  dist.barrier()\n",
        "\n",
        "  print(f\"Data in {rank=} is: {tensor}\")\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()\n",
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "vX8nMIYRFjX8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process for Async P2P communication\n",
        "\n",
        "# 如果需要非阻塞通信，可以使用 isend/irecv\n",
        "# 也可以使用dist.batch_isend_irecv fuse多个P2P通信操作. 该函数会尝试fuse多个NCCL kernel来提高throughput，并re-order通信顺序以减少deadlock概率。\n",
        "# 在通信完成前不要修改发送缓冲区(buffer)，在通信完成前不要使用接收缓冲区，必须等待 wait() 完成后才能安全操作相关数据\n",
        "# 每个异步操作都会占用系统资源，应及时调用 wait() 释放资源\n",
        "# 避免同时发起过多未完成的异步操作\n",
        "# 异步操作可能在后台失败，wait() 调用会暴露通信过程中的错误，建议使用 try-finally 确保资源正确清理\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "def do_other_work(rank: int):\n",
        "  print(f\"Rank {rank} is doing other work ...\")\n",
        "  time.sleep(random.uniform(1.0, 3.0))\n",
        "\n",
        "def init_process_p2p_async(rank: int, world_size: int):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  if rank == 0:\n",
        "    tensor = torch.tensor([100, 200], dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    send_req = dist.isend(tensor, dst=1)\n",
        "    print(f\"Rank 0 sending data to Rank 1 ...\")\n",
        "\n",
        "    do_other_work(rank)\n",
        "\n",
        "    send_req.wait()\n",
        "    print(f\"Rank 0 sent data to Rank 1 ...\")\n",
        "\n",
        "  elif rank == 1:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    recv_req = dist.irecv(tensor, src=0)\n",
        "    print(f\"Rank 1 is receiving data from Rank 0 ...\")\n",
        "\n",
        "    do_other_work(rank)\n",
        "    recv_req.wait()\n",
        "    print(f\"Rank 1 received data from Rank 0 ...\")\n",
        "\n",
        "    tensor += 100\n",
        "    print(f\"Modified data in {rank=} is: {tensor}\")\n",
        "    send_req = dist.isend(tensor, dst=2)\n",
        "    print(f\"Rank 1 is sending data to Rank 2 ...\")\n",
        "\n",
        "    do_other_work(rank)\n",
        "\n",
        "    send_req.wait()\n",
        "    print(f\"Rank 1 sent data to Rank 2 ...\")\n",
        "\n",
        "  elif rank == 2:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "    print(f\"Initial data in {rank=} is: {tensor}\")\n",
        "    recv_req = dist.irecv(tensor, src=1)\n",
        "    print(f\"Rank 2 is receiving data from Rank 1 ...\")\n",
        "\n",
        "    do_other_work(rank)\n",
        "\n",
        "    recv_req.wait()\n",
        "    print(f\"Rank 2 received data from Rank 1 ...\")\n",
        "  else:\n",
        "    tensor = torch.zeros(2, dtype=torch.float32)\n",
        "\n",
        "  # dist.barrier()\n",
        "\n",
        "  print(f\"Data in {rank=} is: {tensor}\")\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()\n",
        ""
      ],
      "metadata": {
        "id": "UQBYoHxSq49q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title all_reduce & all_gather\n",
        "\n",
        "# 功能定位：\n",
        "# all_reduce: 对所有进程的数据进行规约（reduction）操作，如求和、取最大值等\n",
        "# all_gather: 收集所有进程的数据，不进行运算，只是简单合并\n",
        "# 输出结果：\n",
        "# all_reduce: 所有进程得到相同的规约结果\n",
        "# all_gather: 所有进程得到包含所有进程原始数据的完整列表\n",
        "# 内存使用：\n",
        "# all_reduce: 输出张量大小与输入相同\n",
        "# all_gather: 输出张量大小是输入的 world_size 倍\n",
        "# 适用场景：\n",
        "# all_reduce：计算分布式损失，梯度同步，计算全局统计信息（如准确率）\n",
        "# all_gather：获取其他进程的原始数据，分布式评估指标计算，汇总不同进程的中间结果\n",
        "# 通讯效率：\n",
        "# all_reduce 通常比 all_gather 更高效，如果只需要得到最终的汇总结果，应优先使用 all_reduce，传输的数据量更小，可以利用树形结构进行规约。\n",
        "\n",
        "def init_process_all_reduce_n_all_gather(rank, world_size):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  # Test all_gather\n",
        "  tensor = torch.tensor([rank * 10, rank * 10 + 1], dtype=torch.float32)\n",
        "  gathered = [torch.zeros(2, dtype=torch.float) for _ in range(world_size)]\n",
        "  dist.all_gather(gathered, tensor)\n",
        "\n",
        "  if rank == 0:\n",
        "    print(f\"\\n=== all_gather result ===\")\n",
        "    print(f\"Original {tensor=}\")\n",
        "    print(f\"Gathered tensor:\")\n",
        "    for i, t in enumerate(gathered):\n",
        "      print(f\"rank {rank} data = {t}\")\n",
        "  dist.barrier()\n",
        "\n",
        "  # Test all_reduce\n",
        "  reduced_tensor = tensor.clone()\n",
        "  if rank == 0:\n",
        "    print(f\"Before all_reduce: {reduced_tensor}\")\n",
        "  dist.all_reduce(reduced_tensor, op=dist.ReduceOp.SUM)\n",
        "  if rank == 0:\n",
        "    print(f\"\\n=== all_reduce result ===\")\n",
        "    print(reduced_tensor)\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A8hQKPpi9DoU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Softmax\n",
        "\n",
        "# Implement using all_gather & all_reduce\n",
        "def init_process_softmax(rank, world_size):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  torch.manual_seed(rank)\n",
        "\n",
        "  logit = torch.zeros(1, dtype=torch.float32)\n",
        "  logit.uniform_(-10.0, 10.0)\n",
        "\n",
        "  max_logit = logit.clone()\n",
        "  dist.all_reduce(max_logit, op=dist.ReduceOp.MAX)\n",
        "\n",
        "  prob = (logit - max_logit).exp()\n",
        "  prob_sum = prob.clone()\n",
        "  dist.all_reduce(prob_sum, op=dist.ReduceOp.SUM)\n",
        "\n",
        "  norm_prob = prob / prob_sum\n",
        "\n",
        "  print(f\"In {rank=}, logit={logit.item():.3f}, prob={norm_prob.item():.3f}\")\n",
        "\n",
        "  # Assert the softmax sum to 1.\n",
        "  norm_prob_sum = norm_prob.clone()\n",
        "  dist.all_reduce(norm_prob_sum, op=dist.ReduceOp.SUM)\n",
        "  torch.allclose(norm_prob_sum, torch.ones(1))\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "32ExcLgjiv6K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Broadcast\n",
        "\n",
        "# broadcast 将源进程 src 的张量数据广播到所有其他进程的同名张量\n",
        "# 接收数据的进程必须预先分配好相同大小的张量空间\n",
        "# 广播操作是阻塞的，所有进程都需要执行到这行代码才能继续\n",
        "# 数据会直接在预分配的内存上进行修改，而不是创建新的张量\n",
        "\n",
        "# Implement using all_gather & all_reduce\n",
        "def init_process_broadcast(rank, world_size):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  if rank == 0:\n",
        "    tensor_1 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
        "    tensor_2 = torch.zeros(2, dtype=torch.float32)\n",
        "  elif rank == 1:\n",
        "    tensor_1 = torch.zeros(3, dtype=torch.float32)\n",
        "    tensor_2 = torch.tensor([4.0, 5.0], dtype=torch.float32)\n",
        "  else:\n",
        "    tensor_1 = torch.zeros(3, dtype=torch.float32)\n",
        "    tensor_2 = torch.zeros(2, dtype=torch.float32)\n",
        "\n",
        "  print(f\"{rank=}\")\n",
        "\n",
        "  dist.broadcast(tensor_1, src=0)\n",
        "  dist.broadcast(tensor_2, src=1)\n",
        "\n",
        "  print(f\"{rank=}, {tensor_1=}, {tensor_2=}\")\n",
        "\n",
        "  # Finish\n",
        "  print(f\"\\nFinishing process with {rank=}, {world_size=}\")\n",
        "  dist.destroy_process_group()\n"
      ],
      "metadata": {
        "id": "NSTug0XkwxUH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Scatter\n",
        "\n",
        "def init_process_scatter(rank, world_size):\n",
        "  print(f\"Starting process with {rank=}, {world_size=}\")\n",
        "\n",
        "  # Use the gloo backend for CPU-based distributed processing\n",
        "  dist.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
        "\n",
        "  assert rank == dist.get_rank()\n",
        "  assert world_size == dist.get_world_size()\n",
        "  dist.barrier()\n",
        "\n",
        "  output_t = torch.zeros(2, dtype=torch.float32)\n",
        "\n",
        "  if rank == 0:\n",
        "    scatter_list = [torch.ones(2, dtype=torch.float32) * i for i in range(world_size)]\n",
        "  else:\n",
        "    scatter_list = None\n",
        "\n",
        "  dist.scatter(output_t, scatter_list, src=0)\n",
        "\n",
        "  print(f\"{rank=}, {output_t=}\")"
      ],
      "metadata": {
        "id": "XdOJldwf8fYe"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the distributed processing\n",
        "\n",
        "# scatter 是一对多的分发操作，只有源进程(这里是 rank 0)需要准备完整数据\n",
        "\n",
        "# 其他进程的 scatter_list 必须设为 None，这是 PyTorch 的规定\n",
        "\n",
        "# 数据必须事先按进程数量切分好，每个进程获得一份\n",
        "\n",
        "# scatter 操作是同步的，所有进程都会在这里等待，直到通信完成\n",
        "\n",
        "# 必须指定源进程 (src=0)，表明数据从哪个进程分发出去\n",
        "\n",
        "# scatter_list 中的每个张量大小必须相同\n",
        "\n",
        "# 总数据量必须能被进程数整除\n",
        "\n",
        "# scatter 适合将大数据集划分给多个进程处理\n",
        "\n",
        "# 相比 broadcast，scatter 可以节省其他进程的内存使用\n",
        "\n",
        "# scatter 适合：\n",
        "\n",
        "# 数据并行训练时分发不同的数据批次\n",
        "# 将大规模数据集分片到多个节点进行处理\n",
        "# 在参数服务器架构中分发模型参数\n",
        "# 为什么说 scatter 比起 broadcast 节省空间？\n",
        "\n",
        "# 考虑一共 4 个进程，需要从 rank 0 发 [1000, 250] 维度的数据给 rank 1, 2, 3，那么用 broadcast 则每张卡上都得有 [1000, 250] 大小的的数据块，然后各自切片。使用 scatter 则只有 rank 0 上会有 [1000, 1000]，其他 rank 上是 [1000, 250]。\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12359' # You can choose a different port if 12355 is in use\n",
        "\n",
        "world_size = 4\n",
        "\n",
        "processes = []\n",
        "for rank in range(world_size):\n",
        "  p = mp.Process(target=init_process_scatter, args=(rank, world_size))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMkYDkQMFH96",
        "outputId": "9986c679-adf7-4bd8-e074-2d0960658bfd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting process with rank=0, world_size=4Starting process with rank=1, world_size=4Starting process with rank=2, world_size=4\n",
            "\n",
            "\n",
            "Starting process with rank=3, world_size=4\n",
            "rank=3, output_t=tensor([3., 3.])\n",
            "rank=2, output_t=tensor([2., 2.])rank=1, output_t=tensor([1., 1.])\n",
            "rank=0, output_t=tensor([0., 0.])\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
